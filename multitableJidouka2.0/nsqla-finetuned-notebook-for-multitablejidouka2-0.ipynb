{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch  --quiet\n\n# # Install Hugging Face libraries\n!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n\n# #FlashAttention only supports Ampere GPUs or newer. #NEED A100 IN GOOGLE COLAB\n!pip install -U transformers\n# # !pip install -U flash-attn --no-build-isolation --quiet\n\n\n! pip install peft --quiet\n! pip install datasets trl ninja packaging --quiet\n\n# # Uncomment only if you're using A100 GPU\n# #!pip install flash-attn --no-build-isolation\n!pip install diffusers safetensors  --quiet\n\n# %pip install -U wandb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n    DataCollatorForSeq2Seq,\n    EarlyStoppingCallback\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset, DatasetDict\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:29:14.172131Z","iopub.execute_input":"2024-11-20T08:29:14.172544Z","iopub.status.idle":"2024-11-20T08:29:35.363387Z","shell.execute_reply.started":"2024-11-20T08:29:14.172496Z","shell.execute_reply":"2024-11-20T08:29:35.361709Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m      3\u001b[0m     AutoTokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     EarlyStoppingCallback\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     LoraConfig,\n\u001b[1;32m     14\u001b[0m     PeftModel,\n\u001b[1;32m     15\u001b[0m     prepare_model_for_kbit_training,\n\u001b[1;32m     16\u001b[0m     get_peft_model,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, DatasetDict\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peft'"],"ename":"ModuleNotFoundError","evalue":"No module named 'peft'","output_type":"error"}]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on SQL dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = \"phamhai/Llama-3.2-1B-Instruct-Frog\"\nnew_model = \"llama-3.2-1b-sql_finetuned_multitableJidouka_4.0\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_dtype = torch.float16\n\n# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float32,\n    #attn_implementation=attn_implementation\n)\ntokenizer = AutoTokenizer.from_pretrained(base_model,use_fast=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# peft_config = LoraConfig(\n#     r=16,\n#     lora_alpha=16,\n#     lora_dropout=0,\n#     bias=\"none\",\n#     task_type=\"CAUSAL_LM\",\n#     target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj'],\n#     use_rslora=False,\n#     loftq_config=None\n# )\n# model = get_peft_model(model, peft_config)\n\n# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:29:41.428411Z","iopub.execute_input":"2024-11-20T08:29:41.428809Z","iopub.status.idle":"2024-11-20T08:29:41.829241Z","shell.execute_reply.started":"2024-11-20T08:29:41.428771Z","shell.execute_reply":"2024-11-20T08:29:41.828259Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"dataset_train = load_dataset(\"huyhoangt2201/multitableJidouka2.0\", split='train[:90%]')\ndataset_val = load_dataset(\"huyhoangt2201/multitableJidouka2.0\", split='train[-10%:]')\ndataset = DatasetDict({\n    'train': dataset_train,\n    'validation': dataset_val\n})\ndataset.save_to_disk(\"completed_train_dataset\")","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:29:44.338047Z","iopub.execute_input":"2024-11-20T08:29:44.338766Z","iopub.status.idle":"2024-11-20T08:29:50.708051Z","shell.execute_reply.started":"2024-11-20T08:29:44.338723Z","shell.execute_reply":"2024-11-20T08:29:50.706962Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"(…)records_english_multitableJidouka2.0.csv:   0%|          | 0.00/191k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49ff795721024edebb454f6320ff51e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1009 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dbbe0444e464ba29a7029737390d462"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/908 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36531690dffd47739477690e95d683be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/101 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7413ee88f0ac4b93bd3a4ecde902c53e"}},"metadata":{}}]},{"cell_type":"code","source":"prompt_template = \"\"\" \nYou are an SQL query assistant. Based on schema below, generate an SQL query to retrieve the relevant information for the user. If the user’s question is unrelated to the table, respond naturally in user's language.\n\nSchema:\n-- Table: Job\nCREATE TABLE Job (\n    Id INT PRIMARY KEY AUTO_INCREMENT,\n    Job_name NVARCHAR(255) NOT NULL,\n);\n\n-- Table: Department\nCREATE TABLE Department (\n    Id INT PRIMARY KEY AUTO_INCREMENT,\n    Department_name NVARCHAR(255) NOT NULL,\n);\n\n-- Table: Author\nCREATE TABLE Author (\n    Id INT PRIMARY KEY AUTO_INCREMENT,\n    Author_name NVARCHAR(255) NOT NULL,\n);\n\n-- Table: Tool\nCREATE TABLE Tool (\n    Id INT PRIMARY KEY AUTO_INCREMENT,\n    Tool_name NVARCHAR(255) NOT NULL,\n);\n\n-- Table: Jidouka\nCREATE TABLE Jidouka (\n    Id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    Improve_name NVARCHAR(255) NOT NULL,\n    Job_id INT,\n    Department_id INT,\n    Author_id INT,\n    Description NVARCHAR(255),\n    Product_name NVARCHAR(255),\n    Time INT,\n    Applications INT,\n    Release_date DATETIME,\n    Other_info NVARCHAR(255),\n    FOREIGN KEY (Job_id) REFERENCES Job(Id),\n    FOREIGN KEY (Department_id) REFERENCES Department(Id),\n    FOREIGN KEY (Author_id) REFERENCES Author(Id)\n);\n\n-- Table: JidoukaTool\nCREATE TABLE JidoukaTool (\n    Jidouka_id BIGINT,\n    Tool_id INT,\n    PRIMARY KEY (Jidouka_id, Tool_id),\n    FOREIGN KEY (Jidouka_id) REFERENCES Jidouka(Id),\n    FOREIGN KEY (Tool_id) REFERENCES Tool(Id)\n);\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:32:22.708668Z","iopub.execute_input":"2024-11-20T08:32:22.709081Z","iopub.status.idle":"2024-11-20T08:32:22.715428Z","shell.execute_reply.started":"2024-11-20T08:32:22.709044Z","shell.execute_reply":"2024-11-20T08:32:22.714294Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def format_context(sample):\n    sample['context'] = prompt_template\n    return sample","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:33:03.669606Z","iopub.execute_input":"2024-11-20T08:33:03.670033Z","iopub.status.idle":"2024-11-20T08:33:03.675590Z","shell.execute_reply.started":"2024-11-20T08:33:03.670000Z","shell.execute_reply":"2024-11-20T08:33:03.674450Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dataset_train2 = dataset_train.map(format_context,batched=False)\ndataset_val2 = dataset_val.map(format_context, batched=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:34:05.374346Z","iopub.execute_input":"2024-11-20T08:34:05.374904Z","iopub.status.idle":"2024-11-20T08:34:05.466698Z","shell.execute_reply.started":"2024-11-20T08:34:05.374863Z","shell.execute_reply":"2024-11-20T08:34:05.465461Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/908 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba55a0c7f5d54cceb9322c90a216e7dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/101 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"860e215d7b32493397f7df0164602d29"}},"metadata":{}}]},{"cell_type":"code","source":"def format_data_template(sample):\n    chat = [\n          {\"role\":\"system\", \"content\": sample['context']},\n          {\"role\":\"user\", \"content\":sample['question']},\n          {\"role\":\"assistant\",\"content\":sample['answer']}\n    ]\n    return {\n        \"messages\": tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n    }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = dataset_train2.map(format_data_template, remove_columns=['context','question','answer'])\ntest_set = dataset_val2.map(format_data_template, remove_columns=['context', 'question','answer'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping_callback = EarlyStoppingCallback( \n    early_stopping_patience=5\n)\n\ntraining_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    optim=\"adamw_8bit\",\n    num_train_epochs=20,\n    eval_strategy=\"epoch\",\n    eval_steps=0.2,\n    save_strategy='epoch',\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=True,\n    group_by_length=True,\n    report_to=\"wandb\",\n    load_best_model_at_end = True\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset_train,\n    eval_dataset = dataset_valid,\n    dataset_text_field = 'messages',\n    max_seq_length = 2048, \n    peft_config = peft_config, \n    packing=False,\n    args = training_arguments,\n    callbacks=[early_stopping_callback]\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\neot = \"<|eot_id|>\"\neot_id = tokenizer.convert_tokens_to_ids(eot)\ntokenizer.pad_token = eot\ntokenizer.pad_token_id = eot_id\n\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = 'llama-3.2-1b-sql_finetuned_multitableJidouka_4.0_adapter'\nnew_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = 'huyhoangt2201/llama-3.2-1b-sql_finetuned_multitableJidouka_4.0_adapter'\nbase_model = 'phamhai/Llama-3.2-1B-Instruct-Frog'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model)\n\nbase_model_reload = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        return_dict=True,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\n\n# base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n\n# Merge adapter with base model\nmerge_model = PeftModel.from_pretrained(base_model_reload, new_model)\n\nmerge_model = merge_model.merge_and_unload()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model_merged = 'llama-3.2-1b-sql_finetuned_multitableJidouka_4.0_merged'\nmerge_model.save_pretrained(new_model_merged)\ntokenizer.save_pretrained(new_model_merged)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token = hf_token)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merge_model.push_to_hub(new_model_merged, use_temp_dir=False)\ntokenizer.push_to_hub(new_model_merged, use_temp_dir=False)","metadata":{},"execution_count":null,"outputs":[]}]}