{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T03:51:20.299313Z",
     "iopub.status.busy": "2024-11-08T03:51:20.298811Z",
     "iopub.status.idle": "2024-11-08T03:51:54.747143Z",
     "shell.execute_reply": "2024-11-08T03:51:54.745995Z",
     "shell.execute_reply.started": "2024-11-08T03:51:20.299240Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-08T03:51:57.670752Z",
     "iopub.status.busy": "2024-11-08T03:51:57.669861Z",
     "iopub.status.idle": "2024-11-08T03:52:21.541733Z",
     "shell.execute_reply": "2024-11-08T03:52:21.540631Z",
     "shell.execute_reply.started": "2024-11-08T03:51:57.670705Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\chatbot\\envs\\chatbot\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T03:52:25.822481Z",
     "iopub.status.busy": "2024-11-08T03:52:25.821753Z",
     "iopub.status.idle": "2024-11-08T03:52:25.827089Z",
     "shell.execute_reply": "2024-11-08T03:52:25.826169Z",
     "shell.execute_reply.started": "2024-11-08T03:52:25.822440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ['WANDB_DISABLED']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T03:52:27.470839Z",
     "iopub.status.busy": "2024-11-08T03:52:27.470474Z",
     "iopub.status.idle": "2024-11-08T03:52:27.541493Z",
     "shell.execute_reply": "2024-11-08T03:52:27.540198Z",
     "shell.execute_reply.started": "2024-11-08T03:52:27.470802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T03:52:28.485040Z",
     "iopub.status.busy": "2024-11-08T03:52:28.484573Z",
     "iopub.status.idle": "2024-11-08T03:52:28.491345Z",
     "shell.execute_reply": "2024-11-08T03:52:28.489958Z",
     "shell.execute_reply.started": "2024-11-08T03:52:28.484997Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T03:52:30.481893Z",
     "iopub.status.busy": "2024-11-08T03:52:30.481429Z",
     "iopub.status.idle": "2024-11-08T03:52:55.830736Z",
     "shell.execute_reply": "2024-11-08T03:52:55.829934Z",
     "shell.execute_reply.started": "2024-11-08T03:52:30.481848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = 'Biscottezi/vit5-base-finetuned-vitext2sql'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "original_model = original_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-11-08T03:52:57.694402Z",
     "iopub.status.busy": "2024-11-08T03:52:57.693099Z",
     "iopub.status.idle": "2024-11-08T03:52:57.711722Z",
     "shell.execute_reply": "2024-11-08T03:52:57.710482Z",
     "shell.execute_reply.started": "2024-11-08T03:52:57.694336Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(36096, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(36096, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(36096, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=36096, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T03:53:04.786743Z",
     "iopub.status.busy": "2024-11-08T03:53:04.785689Z",
     "iopub.status.idle": "2024-11-08T03:53:06.493588Z",
     "shell.execute_reply": "2024-11-08T03:53:06.492325Z",
     "shell.execute_reply.started": "2024-11-08T03:53:04.786700Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 879/879 [00:00<00:00, 87900.08 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 98/98 [00:00<00:00, 6998.48 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train dataset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_train = load_dataset(\"huyhoangt2201/contextawareJidouka_fixed\", split='train[:90%]')\n",
    "dataset_val = load_dataset(\"huyhoangt2201/contextawareJidouka_fixed\", split='train[-10%:]')\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset_train,\n",
    "    'validation': dataset_val\n",
    "})\n",
    "dataset.save_to_disk(\"completed_train_dataset\")\n",
    "print(\"Created train dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = \"\"\"You are an SQL query assistant. Based on schema, generate an SQL query to retrieve the relevant information for the user. If the user’s question is unrelated to the table, respond naturally in user's language.\n",
    "\n",
    "Schema:\n",
    "+Table Author, columns=[AuthorId: int, AuthorName: nvarchar(255), DepartmentId int, GroupDCId int]\n",
    "+Table Department, columns=[DepartmentId: int, DepartmentName: nvarchar(255)]\n",
    "+Table GroupDC, columns=[GroupDCId: int, DepartmentId: int, GroupDCName nvarchar(255)]\n",
    "+Table Job, columns=[JobId: int, JobName: nvarchar(255)]\n",
    "+Table Tool, columns=[ToolId: int, ToolName: nvarchar(255), ToolDescription: text]\n",
    "+Table Jidouka, columns=[JidoukaId: bigint, ProductApply: nvarchar(255), ImprovementName: nvarchar(255), SoftwareUsing: nvarchar(255), Description: nvarchar(255), Video: text, DetailDocument: text, TotalJobApplied: int, TotalTimeSaved: int, DateCreate: datetime, JobId: int, AuthorId: int, DepartmentId: int, GroupDCId: int]\n",
    "+Table JidoukaTool, columns=[JidoukaId: bigint, ToolId: int]\n",
    "+Primary_keys=[Author.AuthorId, Department.DepartmentId, GroupDC.GroupDCId, Job.JobId, Tool.ToolId, Jidouka.JidoukaId]\n",
    "+Foreign_keys=[GroupDC.DepartmentId=Department.DepartmentId, Jidouka.JobId=Job.JobId, Jidouka.AuthorId=Author.AuthorId, Jidouka.DepartmentId=Department.DepartmentId, Jidouka.GroupDCId=GroupDC.GroupDCId, JidoukaTool.JidoukaId=Jidouka.JidoukaId, JidoukaTool.ToolId=Tool.ToolId, Author.DepartmentId=Department.DepartmentId, Author.GroupDCId=GroupDC.GroupDCId]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(sample):\n",
    "    sample['context'] = system_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train2 = dataset_train.map(format_context)\n",
    "dataset_val2 = dataset_val.map(format_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train3 = dataset_train2.shuffle(seed=42)\n",
    "dataset_val3 = dataset_val2.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(sample):\n",
    "    \"\"\"\n",
    "    Convert dataset to instructions for LLM\n",
    "    Args:\n",
    "    sample: a record from dataset include id, context, questions, sql_answer\n",
    "    \"\"\"\n",
    "    start_prompt = \"Context:\\n\"\n",
    "    middle_prompt = \"\\n\\nQuestion:\\n\"\n",
    "    end_prompt = \"\\n\\nAnswer:\\n\"\n",
    "\n",
    "    data_zip = zip(sample['context'], sample['previous_question'])\n",
    "    prompt = [start_prompt + context + middle_prompt + question + end_prompt for context, question in data_zip]\n",
    "    sample['input_ids'] = tokenizer(prompt, padding=True, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    sample['labels'] = tokenizer(sample['previous_answer'], padding=True, truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['previous_question', 'previous_answer', 'schema_linking', 'question', 'answer', 'context'],\n",
       "    num_rows: 879\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T03:53:13.825423Z",
     "iopub.status.busy": "2024-11-08T03:53:13.824707Z",
     "iopub.status.idle": "2024-11-08T03:53:14.334744Z",
     "shell.execute_reply": "2024-11-08T03:53:14.333701Z",
     "shell.execute_reply.started": "2024-11-08T03:53:13.825380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_datasets_train = dataset_train3.map(tokenize_function, batched=True)\n",
    "tokenized_datasets_train = tokenized_datasets_train.remove_columns(['previous_question', 'previous_answer', 'schema_linking', 'question', 'answer', 'context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets_test = dataset_val3.map(tokenize_function, batched=True)\n",
    "tokenized_datasets_test = tokenized_datasets_test.remove_columns(['previous_question', 'previous_answer', 'schema_linking', 'question', 'answer', 'context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T03:53:30.613189Z",
     "iopub.status.busy": "2024-11-08T03:53:30.612817Z",
     "iopub.status.idle": "2024-11-08T03:53:34.472307Z",
     "shell.execute_reply": "2024-11-08T03:53:34.471178Z",
     "shell.execute_reply.started": "2024-11-08T03:53:30.613151Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'context'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m answer \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mContext:\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124mAnswer:\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'context'"
     ]
    }
   ],
   "source": [
    "question = dataset['validation'][0]['question']\n",
    "context = dataset['validation'][0]['context']\n",
    "answer = dataset['validation'][0]['answer']\n",
    "\n",
    "prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN ANSWER:\\n{answer}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T03:54:18.632632Z",
     "iopub.status.busy": "2024-11-08T03:54:18.632239Z",
     "iopub.status.idle": "2024-11-08T03:54:19.795815Z",
     "shell.execute_reply": "2024-11-08T03:54:19.794489Z",
     "shell.execute_reply.started": "2024-11-08T03:54:18.632593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "to_train = True\n",
    "model_name = 'Biscottezi/vit5-base-finetuned-vitext2sql'\n",
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "finetuned_model = finetuned_model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T16:26:18.885336Z",
     "iopub.status.busy": "2024-11-07T16:26:18.884949Z",
     "iopub.status.idle": "2024-11-07T16:26:19.265085Z",
     "shell.execute_reply": "2024-11-07T16:26:19.264168Z",
     "shell.execute_reply.started": "2024-11-07T16:26:18.885274Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"finetuned_model_10_epoch\")\n",
    "# finetuned_model = finetuned_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T03:55:12.204058Z",
     "iopub.status.busy": "2024-11-08T03:55:12.203321Z",
     "iopub.status.idle": "2024-11-08T04:43:59.926754Z",
     "shell.execute_reply": "2024-11-08T04:43:59.925641Z",
     "shell.execute_reply.started": "2024-11-08T03:55:12.204013Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "  0%|          | 0/5500 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if to_train:\n",
    "    output_dir = f'sql-training-{str(int(time.time()))}'\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=5e-3,\n",
    "        num_train_epochs=100,\n",
    "        per_device_train_batch_size=16,     # batch size per device during training\n",
    "        per_device_eval_batch_size=16,      # batch size for evaluation\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',        # evaluation strategy to adopt during training\n",
    "        eval_steps=50,        \n",
    "        load_best_model_at_end=True             # number of steps between evaluation\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStoppingCallback( \n",
    "        early_stopping_patience=5\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=finetuned_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets_train,\n",
    "        eval_dataset=tokenized_datasets_test,\n",
    "        callbacks=[early_stopping_callback]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    finetuned_model.save_pretrained(\"finetuned_model_100_epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T04:46:05.889217Z",
     "iopub.status.busy": "2024-11-08T04:46:05.888784Z",
     "iopub.status.idle": "2024-11-08T04:46:07.229140Z",
     "shell.execute_reply": "2024-11-08T04:46:07.228047Z",
     "shell.execute_reply.started": "2024-11-08T04:46:05.889174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"finetuned_model_100_epoch\")\n",
    "finetuned_model = finetuned_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T04:47:26.437877Z",
     "iopub.status.busy": "2024-11-08T04:47:26.436789Z",
     "iopub.status.idle": "2024-11-08T04:47:27.158049Z",
     "shell.execute_reply": "2024-11-08T04:47:27.157015Z",
     "shell.execute_reply.started": "2024-11-08T04:47:26.437831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Context:\n",
      "Có 1 bảng cần truy vấn. \n",
      "Bảng cần truy vấn bao gồm các cột: \n",
      "id: số thứ tự của hàng (int);\n",
      "innovation_name: tên của tác phẩm cải tiến (str);\n",
      "task_type: Tác phẩm cải tiến đó sinh ra để làm gì? (str) (ví dụ: Xử lí database, nhập thông tin, tối ưu quy trình làm việc,...) ;\n",
      "tool: Công cụ để thực hiện (str) (ví dụ: Python, Excel, Visual Studio Code, ...);\n",
      "describe_innovation: Mô tả rõ ràng hơn mục đích của công cụ (giải thích rõ hơn cột task_type) (str)  ;\n",
      "product: Output của công cụ có định dạng như thế nào (str) (ví dụ: file csv, file xlsx, ....);\n",
      "pic: Tên người phụ trách quản lí công cụ (str)  ;\n",
      "dc: Phòng ban làm việc của người phụ trách quản lí công cụ (str) (dc1, dc2, dc3, dcd, souko,...);\n",
      "saved_hours: số lượng giờ mà nhờ việc áp dụng cải tiến tiết kiệm được (int);\n",
      "created_at: Thời điểm công cụ này ra mắt (str) (ví dụ: 2024-10-11, 2024-10-10,...);\n",
      "information: Đường link youtube tài liệu hướng dẫn sử dụng công cụ (str)\n",
      "\n",
      "Question:\n",
      "Công cụ nào giúp tôi tiết kiệm thời gian khi làm việc?\n",
      "\n",
      "Answer:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN ANSWER:\n",
      "\n",
      "SELECT innovation_name, saved_hours FROM jidouka ORDER BY saved_hours DESC LIMIT 1;\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "FINE-TUNED MODEL - ZERO SHOT:\n",
      "SELECT innovation_name FROM jidouka WHERE task_type  'Tối ưu quy trình làm việc';\n"
     ]
    }
   ],
   "source": [
    "question = \"Công cụ nào giúp tôi tiết kiệm thời gian khi làm việc?\"\n",
    "context = \"\"\"Có 1 bảng cần truy vấn. \n",
    "Bảng cần truy vấn bao gồm các cột: \n",
    "id: số thứ tự của hàng (int);\n",
    "innovation_name: tên của tác phẩm cải tiến (str);\n",
    "task_type: Tác phẩm cải tiến đó sinh ra để làm gì? (str) (ví dụ: Xử lí database, nhập thông tin, tối ưu quy trình làm việc,...) ;\n",
    "tool: Công cụ để thực hiện (str) (ví dụ: Python, Excel, Visual Studio Code, ...);\n",
    "describe_innovation: Mô tả rõ ràng hơn mục đích của công cụ (giải thích rõ hơn cột task_type) (str)  ;\n",
    "product: Output của công cụ có định dạng như thế nào (str) (ví dụ: file csv, file xlsx, ....);\n",
    "pic: Tên người phụ trách quản lí công cụ (str)  ;\n",
    "dc: Phòng ban làm việc của người phụ trách quản lí công cụ (str) (dc1, dc2, dc3, dcd, souko,...);\n",
    "saved_hours: số lượng giờ mà nhờ việc áp dụng cải tiến tiết kiệm được (int);\n",
    "created_at: Thời điểm công cụ này ra mắt (str) (ví dụ: 2024-10-11, 2024-10-10,...);\n",
    "information: Đường link youtube tài liệu hướng dẫn sử dụng công cụ (str)\"\"\"\n",
    "answer = \"\"\"\n",
    "SELECT innovation_name, saved_hours FROM jidouka ORDER BY saved_hours DESC LIMIT 1;\"\"\"\n",
    "prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "output = tokenizer.decode(\n",
    "    finetuned_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN ANSWER:\\n{answer}\\n')\n",
    "print(dash_line)\n",
    "print(f'FINE-TUNED MODEL - ZERO SHOT:\\n{output}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6032626,
     "sourceId": 9834962,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
