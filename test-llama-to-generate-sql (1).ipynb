{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch tensorboard --quiet\n\n# Install Hugging Face libraries\n!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n\n#FlashAttention only supports Ampere GPUs or newer. #NEED A100 IN GOOGLE COLAB\n#!pip install -U transformers\n# !pip install -U flash-attn --no-build-isolation --quiet\n\n\n! pip install peft --quiet\n! pip install datasets trl ninja packaging --quiet\n\n# Uncomment only if you're using A100 GPU\n#!pip install flash-attn --no-build-isolation\n!pip install diffusers safetensors  --quiet\n\n%pip install -U wandb","metadata":{"execution":{"iopub.status.busy":"2024-11-12T13:21:28.878407Z","iopub.execute_input":"2024-11-12T13:21:28.878831Z","iopub.status.idle":"2024-11-12T13:22:39.632976Z","shell.execute_reply.started":"2024-11-12T13:21:28.878788Z","shell.execute_reply":"2024-11-12T13:22:39.631908Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.6)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.15.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /opt/conda/lib/python3.10/site-packages (from wandb) (4.12.2)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-11-12T13:23:29.387654Z","iopub.execute_input":"2024-11-12T13:23:29.388929Z","iopub.status.idle":"2024-11-12T13:23:29.394515Z","shell.execute_reply.started":"2024-11-12T13:23:29.388883Z","shell.execute_reply":"2024-11-12T13:23:29.393456Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict","metadata":{"execution":{"iopub.status.busy":"2024-11-12T13:23:32.723387Z","iopub.execute_input":"2024-11-12T13:23:32.724029Z","iopub.status.idle":"2024-11-12T13:23:32.727931Z","shell.execute_reply.started":"2024-11-12T13:23:32.723987Z","shell.execute_reply":"2024-11-12T13:23:32.727029Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\ndevice = torch.device('cuda')\nmodel_path = 'phamhai/Llama-3.2-1B-Instruct-Frog'\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path).to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:01:46.892102Z","iopub.execute_input":"2024-11-12T15:01:46.892454Z","iopub.status.idle":"2024-11-12T15:01:51.194367Z","shell.execute_reply.started":"2024-11-12T15:01:46.892421Z","shell.execute_reply":"2024-11-12T15:01:51.193344Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF_TOKEN2\")\n\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on SQL dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:01:55.231974Z","iopub.execute_input":"2024-11-12T15:01:55.232348Z","iopub.status.idle":"2024-11-12T15:01:58.878694Z","shell.execute_reply.started":"2024-11-12T15:01:55.232312Z","shell.execute_reply":"2024-11-12T15:01:58.877684Z"},"trusted":true},"execution_count":125,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:tlvni8hf) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">ethereal-bird-4</strong> at: <a href='https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset/runs/tlvni8hf' target=\"_blank\">https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset/runs/tlvni8hf</a><br/> View project at: <a href='https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset' target=\"_blank\">https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241112_140443-tlvni8hf/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:tlvni8hf). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241112_150155-cfudwu64</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset/runs/cfudwu64' target=\"_blank\">faithful-water-5</a></strong> to <a href='https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset' target=\"_blank\">https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset/runs/cfudwu64' target=\"_blank\">https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset/runs/cfudwu64</a>"},"metadata":{}}]},{"cell_type":"code","source":"base_model = \"phamhai/Llama-3.2-1B-Instruct-Frog\"\ndataset_path = \"/kaggle/input/sql-dataset/train_dataset.json\"\nnew_model = \"llama-3.2-1b-chat-sql3\"","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:02:03.344279Z","iopub.execute_input":"2024-11-12T15:02:03.345165Z","iopub.status.idle":"2024-11-12T15:02:03.350358Z","shell.execute_reply.started":"2024-11-12T15:02:03.345122Z","shell.execute_reply":"2024-11-12T15:02:03.349499Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"torch_dtype = torch.float16","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:02:04.827799Z","iopub.execute_input":"2024-11-12T15:02:04.828633Z","iopub.status.idle":"2024-11-12T15:02:04.834222Z","shell.execute_reply.started":"2024-11-12T15:02:04.828581Z","shell.execute_reply":"2024-11-12T15:02:04.833175Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n    #attn_implementation=attn_implementation\n)\ntokenizer = AutoTokenizer.from_pretrained(base_model,use_fast=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:02:06.503468Z","iopub.execute_input":"2024-11-12T15:02:06.503841Z","iopub.status.idle":"2024-11-12T15:02:10.942096Z","shell.execute_reply.started":"2024-11-12T15:02:06.503803Z","shell.execute_reply":"2024-11-12T15:02:10.941055Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:02:10.943918Z","iopub.execute_input":"2024-11-12T15:02:10.944254Z","iopub.status.idle":"2024-11-12T15:02:11.244290Z","shell.execute_reply.started":"2024-11-12T15:02:10.944221Z","shell.execute_reply":"2024-11-12T15:02:11.243285Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"# train_set = load_dataset('huyhoangt2201/jidouka3.1',split=\"train\")\n# test_set = load_dataset('huyhoangt2201/jidouka3.1',split='test')\ndataset_train = load_dataset(\"huyhoangt2201/jidouka3.1\", split='train[:90%]')\ndataset_val = load_dataset(\"huyhoangt2201/jidouka3.1\", split='train[-10%:]')\ndataset = DatasetDict({\n    'train': dataset_train,\n    'validation': dataset_val\n})\ndataset.save_to_disk(\"completed_train_dataset\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:02:11.245641Z","iopub.execute_input":"2024-11-12T15:02:11.246137Z","iopub.status.idle":"2024-11-12T15:02:13.736487Z","shell.execute_reply.started":"2024-11-12T15:02:11.246090Z","shell.execute_reply":"2024-11-12T15:02:13.735588Z"},"trusted":true},"execution_count":130,"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1132 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"466ce1b17c904390bd837cabe5b662bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/126 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2088976f0314690bad1fc67ee1606e1"}},"metadata":{}}]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=10,\n    eval_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:02:13.738119Z","iopub.execute_input":"2024-11-12T15:02:13.738416Z","iopub.status.idle":"2024-11-12T15:02:13.772426Z","shell.execute_reply.started":"2024-11-12T15:02:13.738384Z","shell.execute_reply":"2024-11-12T15:02:13.771646Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:32:20.074898Z","iopub.execute_input":"2024-11-12T14:32:20.075286Z","iopub.status.idle":"2024-11-12T14:32:20.082223Z","shell.execute_reply.started":"2024-11-12T14:32:20.075250Z","shell.execute_reply":"2024-11-12T14:32:20.081365Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"{'context': 'Có 1 bảng tên jidouka cần truy vấn. Bảng cần truy vấn bao gồm các cột: \\nid: số thứ tự của hàng (int);\\ntên_cải_tiến: tên của tác phẩm cải tiến (str);\\nloại_hình_công_việc: loại hình công việc mà tác phẩm đó cải tiến? (str) (ví dụ: Xử lí database, nhập thông tin, tối ưu quy trình làm việc,...) ;\\ncông_cụ: Công cụ hỗ trợ để thực hiện mục đích cải tiến (str) (ví dụ: Python, Excel, Visual Studio Code, ...);\\nmô_tả: Mô tả cụ thể chi tiết cải tiến (str) (ví dụ: từng bước thực hiện cải tiến đó) ;\\nsản_phẩm: sản phẩm đầu ra của tác phẩm cải tiến đó (str) (ví dụ: file csv, file xlsx, ....);\\ntác_giả: người đóng góp, nhân viên trong công ty, người đã tạo ra cải tiến đó (str)  ;\\nbộ_phận: Phòng ban làm việc của tác_giả, thường gọi là dc nào đó  (str) (ví dụ: dc1, dc2, dc3, dcd, souko,...);\\nsố_giờ: số lượng giờ mà nhờ việc áp dụng cải tiến tiết kiệm được (int);\\nsố_công_việc_áp_dụng: số công việc trong công ty mà cải tiến đó đã giúp được (int)\\nthời_điểm_ra_mắt: Thời điểm công cụ này ra mắt (str) (ví dụ: 2024-10-11, 2024-10-09,...);\\nthông_tin_thêm: Đường link tài liệu hướng dẫn (powerpoint, video) sử dụng cải tiến, công cụ của cải tiến (str)',\n 'question': \"Cải tiến nào có số giờ tiết kiệm từ 5 đến 15 và có mô tả chứa từ 'tự động hóa báo cáo'?\",\n 'answer': \"SELECT tên_cải_tiến FROM jidouka WHERE số_giờ BETWEEN 5 AND 15 AND mô_tả LIKE '%tự động hóa báo cáo%';\"}"},"metadata":{}}]},{"cell_type":"code","source":"schema = \"\"\"\nBảng jidouka: \n    id: số thứ tự của hàng (int)\n    tên_cải_tiến: tên của tác phẩm cải tiến (str)\n    loại_hình_công_việc: loại hình công việc mà tác phẩm đó cải tiến? (str) (ví dụ: Xử lí database, nhập thông tin, tối ưu quy trình làm việc,...) \n    công_cụ: Công cụ hỗ trợ để thực hiện mục đích cải tiến (str) (ví dụ: Python, Excel, Visual Studio Code, ...);\n    mô_tả: Mô tả cụ thể chi tiết cải tiến (str) (ví dụ: từng bước thực hiện cải tiến đó) ;\n    sản_phẩm: sản phẩm đầu ra của tác phẩm cải tiến đó (str) (ví dụ: file csv, file xlsx, ....);\n    tác_giả: người đóng góp, nhân viên trong công ty, người đã tạo ra cải tiến đó (str)  ;\n    bộ_phận: Phòng ban làm việc của tác_giả, thường gọi là dc nào đó  (str) (ví dụ: dc1, dc2, dc3, dcd, souko,...);\n    số_giờ: số lượng giờ mà nhờ việc áp dụng cải tiến tiết kiệm được (int);\n    số_công_việc_áp_dụng: số công việc trong công ty mà cải tiến đó đã giúp được (int)\n    thời_điểm_ra_mắt: Thời điểm công cụ này ra mắt (str) (ví dụ: 2024-10-11, 2024-10-09,...);\n    thông_tin_thêm: Đường link tài liệu hướng dẫn (powerpoint, video) sử dụng cải tiến, công cụ của cải tiến (str)',\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:35:26.107188Z","iopub.execute_input":"2024-11-12T14:35:26.107914Z","iopub.status.idle":"2024-11-12T14:35:26.113249Z","shell.execute_reply.started":"2024-11-12T14:35:26.107870Z","shell.execute_reply":"2024-11-12T14:35:26.112336Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:48:03.525163Z","iopub.execute_input":"2024-11-12T14:48:03.525882Z","iopub.status.idle":"2024-11-12T14:48:03.532061Z","shell.execute_reply.started":"2024-11-12T14:48:03.525816Z","shell.execute_reply":"2024-11-12T14:48:03.531150Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"{'context': 'Có 1 bảng tên jidouka cần truy vấn. Bảng cần truy vấn bao gồm các cột: \\nid: số thứ tự của hàng (int);\\ntên_cải_tiến: tên của tác phẩm cải tiến (str);\\nloại_hình_công_việc: loại hình công việc mà tác phẩm đó cải tiến? (str) (ví dụ: Xử lí database, nhập thông tin, tối ưu quy trình làm việc,...) ;\\ncông_cụ: Công cụ hỗ trợ để thực hiện mục đích cải tiến (str) (ví dụ: Python, Excel, Visual Studio Code, ...);\\nmô_tả: Mô tả cụ thể chi tiết cải tiến (str) (ví dụ: từng bước thực hiện cải tiến đó) ;\\nsản_phẩm: sản phẩm đầu ra của tác phẩm cải tiến đó (str) (ví dụ: file csv, file xlsx, ....);\\ntác_giả: người đóng góp, nhân viên trong công ty, người đã tạo ra cải tiến đó (str)  ;\\nbộ_phận: Phòng ban làm việc của tác_giả, thường gọi là dc nào đó  (str) (ví dụ: dc1, dc2, dc3, dcd, souko,...);\\nsố_giờ: số lượng giờ mà nhờ việc áp dụng cải tiến tiết kiệm được (int);\\nsố_công_việc_áp_dụng: số công việc trong công ty mà cải tiến đó đã giúp được (int)\\nthời_điểm_ra_mắt: Thời điểm công cụ này ra mắt (str) (ví dụ: 2024-10-11, 2024-10-09,...);\\nthông_tin_thêm: Đường link tài liệu hướng dẫn (powerpoint, video) sử dụng cải tiến, công cụ của cải tiến (str)',\n 'question': \"Cải tiến nào có số giờ tiết kiệm từ 5 đến 15 và có mô tả chứa từ 'tự động hóa báo cáo'?\",\n 'answer': \"SELECT tên_cải_tiến FROM jidouka WHERE số_giờ BETWEEN 5 AND 15 AND mô_tả LIKE '%tự động hóa báo cáo%';\"}"},"metadata":{}}]},{"cell_type":"code","source":"def format_context_to_schema_dataset(sample):\n    schema = \"\"\"\nBảng jidouka: \n    id: số thứ tự của hàng (int)\n    tên_cải_tiến: tên của tác phẩm cải tiến (str)\n    loại_hình_công_việc: loại hình công việc mà tác phẩm đó cải tiến? (str) (ví dụ: Xử lí database, nhập thông tin, tối ưu quy trình làm việc,...) \n    công_cụ: Công cụ hỗ trợ để thực hiện mục đích cải tiến (str) (ví dụ: Python, Excel, Visual Studio Code, ...);\n    mô_tả: Mô tả cụ thể chi tiết cải tiến (str) (ví dụ: từng bước thực hiện cải tiến đó) ;\n    sản_phẩm: sản phẩm đầu ra của tác phẩm cải tiến đó (str) (ví dụ: file csv, file xlsx, ....);\n    tác_giả: người đóng góp, nhân viên trong công ty, người đã tạo ra cải tiến đó (str)  ;\n    bộ_phận: Phòng ban làm việc của tác_giả, thường gọi là dc nào đó  (str) (ví dụ: dc1, dc2, dc3, dcd, souko,...);\n    số_giờ: số lượng giờ mà nhờ việc áp dụng cải tiến tiết kiệm được (int);\n    số_công_việc_áp_dụng: số công việc trong công ty mà cải tiến đó đã giúp được (int)\n    thời_điểm_ra_mắt: Thời điểm công cụ này ra mắt (str) (ví dụ: 2024-10-11, 2024-10-09,...);\n    thông_tin_thêm: Đường link tài liệu hướng dẫn (powerpoint, video) sử dụng cải tiến, công cụ của cải tiến (str)',\n\"\"\"\n    sample['context'] = schema\n    return sample","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:55:40.955532Z","iopub.execute_input":"2024-11-12T14:55:40.956552Z","iopub.status.idle":"2024-11-12T14:55:40.962017Z","shell.execute_reply.started":"2024-11-12T14:55:40.956509Z","shell.execute_reply":"2024-11-12T14:55:40.961156Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"dataset_change_train = dataset['train'].map(format_context_to_schema_dataset)\ndataset_change_valid = dataset['validation'].map(format_context_to_schema_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:55:44.179521Z","iopub.execute_input":"2024-11-12T14:55:44.180376Z","iopub.status.idle":"2024-11-12T14:55:44.303241Z","shell.execute_reply.started":"2024-11-12T14:55:44.180310Z","shell.execute_reply":"2024-11-12T14:55:44.302385Z"},"trusted":true},"execution_count":96,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1132 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90326b6f7c214da2af2d73f1cab5fa10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/126 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4297ccf8b0604c38911f7a6aa171dbe1"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_change_train[0]","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:55:52.223381Z","iopub.execute_input":"2024-11-12T14:55:52.223758Z","iopub.status.idle":"2024-11-12T14:55:52.230385Z","shell.execute_reply.started":"2024-11-12T14:55:52.223718Z","shell.execute_reply":"2024-11-12T14:55:52.229507Z"},"trusted":true},"execution_count":98,"outputs":[{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"{'context': \"\\nBảng jidouka: \\n    id: số thứ tự của hàng (int)\\n    tên_cải_tiến: tên của tác phẩm cải tiến (str)\\n    loại_hình_công_việc: loại hình công việc mà tác phẩm đó cải tiến? (str) (ví dụ: Xử lí database, nhập thông tin, tối ưu quy trình làm việc,...) \\n    công_cụ: Công cụ hỗ trợ để thực hiện mục đích cải tiến (str) (ví dụ: Python, Excel, Visual Studio Code, ...);\\n    mô_tả: Mô tả cụ thể chi tiết cải tiến (str) (ví dụ: từng bước thực hiện cải tiến đó) ;\\n    sản_phẩm: sản phẩm đầu ra của tác phẩm cải tiến đó (str) (ví dụ: file csv, file xlsx, ....);\\n    tác_giả: người đóng góp, nhân viên trong công ty, người đã tạo ra cải tiến đó (str)  ;\\n    bộ_phận: Phòng ban làm việc của tác_giả, thường gọi là dc nào đó  (str) (ví dụ: dc1, dc2, dc3, dcd, souko,...);\\n    số_giờ: số lượng giờ mà nhờ việc áp dụng cải tiến tiết kiệm được (int);\\n    số_công_việc_áp_dụng: số công việc trong công ty mà cải tiến đó đã giúp được (int)\\n    thời_điểm_ra_mắt: Thời điểm công cụ này ra mắt (str) (ví dụ: 2024-10-11, 2024-10-09,...);\\n    thông_tin_thêm: Đường link tài liệu hướng dẫn (powerpoint, video) sử dụng cải tiến, công cụ của cải tiến (str)',\\n\",\n 'question': \"Cải tiến nào có số giờ tiết kiệm từ 5 đến 15 và có mô tả chứa từ 'tự động hóa báo cáo'?\",\n 'answer': \"SELECT tên_cải_tiến FROM jidouka WHERE số_giờ BETWEEN 5 AND 15 AND mô_tả LIKE '%tự động hóa báo cáo%';\"}"},"metadata":{}}]},{"cell_type":"code","source":"system_message = \"\"\"Bạn là một trợ lí ảo thông minh hiểu biết về SQL. User sẽ hỏi bạn câu hỏi bằng tiếng Việt và bạn sẽ tạo ra câu truy vấn SQL dựa vào SCHEMA được cho dưới đây.\nSCHEMA:\n{schema}\n\"\"\"\ndef format_data_template(sample):\n    return {\n      \"messages\": [\n          {\"role\":\"system\", \"content\": system_message.format(schema=sample['context'])},\n          {\"role\":\"user\", \"content\":sample['question']},\n          {\"role\":\"assistant\",\"content\":sample['answer']}\n      ]\n  }","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:58:03.616274Z","iopub.execute_input":"2024-11-12T14:58:03.617026Z","iopub.status.idle":"2024-11-12T14:58:03.622293Z","shell.execute_reply.started":"2024-11-12T14:58:03.616984Z","shell.execute_reply":"2024-11-12T14:58:03.621454Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"dataset['train']","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:57:40.032235Z","iopub.execute_input":"2024-11-12T14:57:40.032618Z","iopub.status.idle":"2024-11-12T14:57:40.038786Z","shell.execute_reply.started":"2024-11-12T14:57:40.032581Z","shell.execute_reply":"2024-11-12T14:57:40.037891Z"},"trusted":true},"execution_count":106,"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['context', 'question', 'answer'],\n    num_rows: 1132\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset_train = dataset_change_train.map(format_data_template, remove_columns = ['context','question','answer'], batched=False)\ndataset_valid = dataset_change_valid.map(format_data_template, remove_columns = ['context', 'question', 'answer'], batched=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:02:47.827561Z","iopub.execute_input":"2024-11-12T15:02:47.827950Z","iopub.status.idle":"2024-11-12T15:02:47.840070Z","shell.execute_reply.started":"2024-11-12T15:02:47.827914Z","shell.execute_reply":"2024-11-12T15:02:47.839165Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"dataset_train","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:02:50.207931Z","iopub.execute_input":"2024-11-12T15:02:50.208307Z","iopub.status.idle":"2024-11-12T15:02:50.215087Z","shell.execute_reply.started":"2024-11-12T15:02:50.208270Z","shell.execute_reply":"2024-11-12T15:02:50.214289Z"},"trusted":true},"execution_count":136,"outputs":[{"execution_count":136,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['messages'],\n    num_rows: 1132\n})"},"metadata":{}}]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset_train,\n    eval_dataset=dataset_valid,\n    peft_config=peft_config,\n    max_seq_length=512,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    dataset_text_field='messages',\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:02:52.431323Z","iopub.execute_input":"2024-11-12T15:02:52.431695Z","iopub.status.idle":"2024-11-12T15:02:54.148348Z","shell.execute_reply.started":"2024-11-12T15:02:52.431659Z","shell.execute_reply":"2024-11-12T15:02:54.147619Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1132 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cec36b202e3849c19d5d587c69355fc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/126 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af6381d9e9864edd8531bc435ebb8061"}},"metadata":{}}]},{"cell_type":"code","source":"eot = \"<|eot_id|>\"\neot_id = tokenizer.convert_tokens_to_ids(eot)\ntokenizer.pad_token = eot\ntokenizer.pad_token_id = eot_id","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:02:56.260335Z","iopub.execute_input":"2024-11-12T15:02:56.260706Z","iopub.status.idle":"2024-11-12T15:02:56.266115Z","shell.execute_reply.started":"2024-11-12T15:02:56.260669Z","shell.execute_reply":"2024-11-12T15:02:56.265150Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:02:58.043742Z","iopub.execute_input":"2024-11-12T15:02:58.044123Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5560' max='5660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5560/5660 56:01 < 01:00, 1.65 it/s, Epoch 9.82/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1132</td>\n      <td>0.031200</td>\n      <td>0.055496</td>\n    </tr>\n    <tr>\n      <td>2264</td>\n      <td>0.031000</td>\n      <td>0.056441</td>\n    </tr>\n    <tr>\n      <td>3396</td>\n      <td>0.024600</td>\n      <td>0.057439</td>\n    </tr>\n    <tr>\n      <td>4528</td>\n      <td>0.021200</td>\n      <td>0.065154</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = '/kaggle/working/llama-3.2-1b-chat-sql3'\nbase_model = 'phamhai/Llama-3.2-1B-Instruct-Frog'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model)\n\nbase_model_reload = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        return_dict=True,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\n\n# base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n\n# Merge adapter with base model\nmerge_model = PeftModel.from_pretrained(base_model_reload, new_model)\n\nmerge_model = model.merge_and_unload()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merge_model.save_pretrained('llama-3.2-1b-chat-sql-base2')\ntokenizer.save_pretrained('llama-3.2-1b-chat-sql-base2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token = hf_token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merge_model.push_to_hub(\"llama-3.2-1b-chat-sql-base2\", use_temp_dir=False)\ntokenizer.push_to_hub(\"llama-3.2-1b-chat-sql-base2\", use_temp_dir=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\ndevice = torch.device('cuda')\nmodel_path = 'huyhoangt2201/llama-3.2-1b-chat-sql-base1'\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path).to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:20:43.387507Z","iopub.execute_input":"2024-11-12T14:20:43.388133Z","iopub.status.idle":"2024-11-12T14:21:12.732648Z","shell.execute_reply.started":"2024-11-12T14:20:43.388081Z","shell.execute_reply":"2024-11-12T14:21:12.731745Z"},"trusted":true},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fa5e82d91084c1b91103894b81485e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45e2fd9e186d4326a1d0d7b94fb07574"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43221bd407e842c680aa55755fd8263e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef83f2927b264dd9bcde253ea00a1902"}},"metadata":{}},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58f5e367ed774e15b7d2d52076f7648b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b40f83bb8b1e4d6090574332ef0a0777"}},"metadata":{}}]},{"cell_type":"code","source":"prompt_template=\"\"\" Bạn là một trợ lí về truy vấn SQL. Dựa vào thông tin có trong schema dưới đây, hãy tạo ra câu lệnh sql để truy vấn ra kết quả phù hợp.\nCâu lệnh sql phải có chính xác tên của các cột trong bảng trong schema, tuyệt đối không gen ra tên cột, bảng không có trong schem.\nNếu một câu hỏi không liên quan tới schema, hãy thừa nhận rằng mình không biết, thay vì cố trả lời sai. Hãy đưa ra câu trả lời logic và thích hợp nhất.\n              Schema:\n              <START_OF_SCHEMA>\n              create table jidouka (\n                id int not null primary key auto_increment,\n                tên_cải_tiến nvarchar(50) not null,\n                loại_hình_công_việc nvarchar(20) not null,\n                công_cụ nvarchar(50) not null,\n                mô_tả_cải_tiến text,\n                phần_mềm nvarchar(50),\n                sản_phảm nvarchar(50),\n                người_đóng_góp nvarchar(30),\n                bộ_phận(dc) nvarchar(10),\n                số_lượng_công_việc_được_cải_tiến int,\n                số_giờ_tiết_kiệm_được int,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                thông_tin_thêm text)\n              <END_OF_SCHEMA>\n                \"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:24:45.150658Z","iopub.execute_input":"2024-11-12T14:24:45.151558Z","iopub.status.idle":"2024-11-12T14:24:45.156899Z","shell.execute_reply.started":"2024-11-12T14:24:45.151514Z","shell.execute_reply":"2024-11-12T14:24:45.156023Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"from typing import List, Dict\nclass ContextAwareChatbot:\n    def __init__(self,prompt, max_history: int = 5):\n        self.model = model\n        self.tokenizer = tokenizer \n        self.max_history = max_history\n        self.conversation_history: List[Dict[str, str]] = []\n        self.prompt=prompt\n    def _build_prompt(self) -> str:\n        # Build context from history\n\n        return self.prompt\n\n    def _clean_response(self, response: str) -> str:\n        # Clean up the generated response\n        response = response.split(\"Assistant:\")[-1].strip()\n        # Stop at any new \"Human:\" or \"Assistant:\" markers\n        if \"Human:\" in response:\n            response = response.split(\"Human:\")[0].strip()\n        return response\n\n    def chat(self, user_input: str) -> str:\n        # Generate the contextualized prompt\n        prompt = self._build_prompt()\n\n#         # Generate response\n#         response = self.pipeline(\n#             prompt,\n#             return_full_text=False,\n#             clean_up_tokenization_spaces=True\n#         )[0]['generated_text']\n\n#         # Clean the response\n#         cleaned_response = self._clean_response(response)\n        messages =[\n            {'role':'system',\n             'content':prompt}\n            ,\n            {'role':'user',\n             'content':user_input}\n        ]\n        tokenized_chat = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n        outputs = self.model.generate(tokenized_chat, max_new_tokens=256).to('cuda')\n        bot_response = self.tokenizer.decode(outputs[0])\n        bot_response = bot_response.split('<|start_header_id|>assistant<|end_header_id|>')\n        bot_response = bot_response[1].strip()[:-10]\n        # Update conversation history\n        self.conversation_history.append({\n            'human': user_input,\n            'assistant': bot_response\n        })\n\n        return bot_response\n\n    def get_history(self) -> List[Dict[str, str]]:\n        return self.conversation_history\n\n    def clear_history(self):\n        self.conversation_history = []\n\n# 4. Create chatbot instance\nchatbot = ContextAwareChatbot(prompt_template)\n\n# 5. Example usage function\ndef chat_session():\n    print(\"Chatbot initialized. Type 'exit' to end the conversation, 'clear' to clear history.\")\n\n    while True:\n        user_input = input(\"\\nYou: \").strip()\n\n        if user_input.lower() == 'exit':\n            print(\"Goodbye!\")\n            break\n        elif user_input.lower() == 'clear':\n            chatbot.clear_history()\n            print(\"Conversation history cleared!\")\n            continue\n\n        response = chatbot.chat(user_input)\n        print(f\"\\nAssistant: {response}\")\n\n# 6. Example of how to use\nif __name__ == \"__main__\":\n    chat_session()","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:24:52.686882Z","iopub.execute_input":"2024-11-12T14:24:52.687590Z","iopub.status.idle":"2024-11-12T14:30:44.193575Z","shell.execute_reply.started":"2024-11-12T14:24:52.687542Z","shell.execute_reply":"2024-11-12T14:30:44.191888Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Chatbot initialized. Type 'exit' to end the conversation, 'clear' to clear history.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nYou:  xin chào\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: điểm tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết tiết\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nYou:  bạn bị sao thế\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: Điểm cụ thể cần cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể cụ thể\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[55], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# 6. Example of how to use\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[43mchat_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[55], line 69\u001b[0m, in \u001b[0;36mchat_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot initialized. Type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to end the conversation, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to clear history.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"],"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error"}]}]}