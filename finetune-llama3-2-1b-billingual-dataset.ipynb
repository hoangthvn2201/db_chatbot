{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch  --quiet\n\n# # Install Hugging Face libraries\n!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n\n# #FlashAttention only supports Ampere GPUs or newer. #NEED A100 IN GOOGLE COLAB\n!pip install -U transformers\n# # !pip install -U flash-attn --no-build-isolation --quiet\n\n\n! pip install peft --quiet\n! pip install datasets trl ninja packaging --quiet\n\n# # Uncomment only if you're using A100 GPU\n# #!pip install flash-attn --no-build-isolation\n!pip install diffusers safetensors  --quiet\n\n# %pip install -U wandb","metadata":{"execution":{"iopub.status.busy":"2024-11-14T05:00:56.263329Z","iopub.execute_input":"2024-11-14T05:00:56.263669Z","iopub.status.idle":"2024-11-14T05:02:31.625893Z","shell.execute_reply.started":"2024-11-14T05:00:56.263636Z","shell.execute_reply":"2024-11-14T05:02:31.624700Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset, DatasetDict\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-11-14T05:02:31.627852Z","iopub.execute_input":"2024-11-14T05:02:31.628193Z","iopub.status.idle":"2024-11-14T05:02:57.448672Z","shell.execute_reply.started":"2024-11-14T05:02:31.628157Z","shell.execute_reply":"2024-11-14T05:02:57.447707Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\ndevice = torch.device('cuda'if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-11-14T05:02:57.449841Z","iopub.execute_input":"2024-11-14T05:02:57.450456Z","iopub.status.idle":"2024-11-14T05:02:57.502769Z","shell.execute_reply.started":"2024-11-14T05:02:57.450420Z","shell.execute_reply":"2024-11-14T05:02:57.501833Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on SQL dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T05:02:57.505646Z","iopub.execute_input":"2024-11-14T05:02:57.506018Z","iopub.status.idle":"2024-11-14T05:03:02.286751Z","shell.execute_reply.started":"2024-11-14T05:02:57.505985Z","shell.execute_reply":"2024-11-14T05:03:02.285991Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuyhoangt2201\u001b[0m (\u001b[33mhuyhoangt2201-fpt-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111388302222167, max=1.0)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d5897b7dd2c4fa28bda172c38db943e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241114_050258-97nm4wq0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset/runs/97nm4wq0' target=\"_blank\">dainty-haze-10</a></strong> to <a href='https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset' target=\"_blank\">https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset/runs/97nm4wq0' target=\"_blank\">https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset/runs/97nm4wq0</a>"},"metadata":{}}]},{"cell_type":"code","source":"base_model = \"phamhai/Llama-3.2-1B-Instruct-Frog\"\nnew_model = \"llama-3.2-1b-sql_finetuned_billingual_3.0\"","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:55:20.894850Z","iopub.execute_input":"2024-11-14T08:55:20.895212Z","iopub.status.idle":"2024-11-14T08:55:20.899731Z","shell.execute_reply.started":"2024-11-14T08:55:20.895180Z","shell.execute_reply":"2024-11-14T08:55:20.898746Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"torch_dtype = torch.float16\n\n# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float32\n    #attn_implementation=attn_implementation\n)\ntokenizer = AutoTokenizer.from_pretrained(base_model,use_fast=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:55:36.540833Z","iopub.execute_input":"2024-11-14T08:55:36.541742Z","iopub.status.idle":"2024-11-14T08:55:41.252368Z","shell.execute_reply.started":"2024-11-14T08:55:36.541699Z","shell.execute_reply":"2024-11-14T08:55:41.251561Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:55:41.684522Z","iopub.execute_input":"2024-11-14T08:55:41.685198Z","iopub.status.idle":"2024-11-14T08:55:41.972281Z","shell.execute_reply.started":"2024-11-14T08:55:41.685163Z","shell.execute_reply":"2024-11-14T08:55:41.971478Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"dataset_train = load_dataset(\"huyhoangt2201/Jidouka3.2\", split='train[:90%]')\ndataset_val = load_dataset(\"huyhoangt2201/Jidouka3.2\", split='train[-10%:]')\ndataset = DatasetDict({\n    'train': dataset_train,\n    'validation': dataset_val\n})\ndataset.save_to_disk(\"completed_train_dataset\")","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:55:45.027568Z","iopub.execute_input":"2024-11-14T08:55:45.028423Z","iopub.status.idle":"2024-11-14T08:55:46.551953Z","shell.execute_reply.started":"2024-11-14T08:55:45.028385Z","shell.execute_reply":"2024-11-14T08:55:46.551078Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/936 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b46e1c8579bf4099927b782b009abef8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/104 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff9251db5ecd419bb255584ef059a677"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_train2 = load_dataset(\"huyhoangt2201/jidouka3.1\", split='train[:90%]')\ndataset_val2 = load_dataset(\"huyhoangt2201/jidouka3.1\", split='train[-10%:]')\ndataset2 = DatasetDict({\n    'train': dataset_train2,\n    'validation': dataset_val2\n})","metadata":{"execution":{"iopub.status.busy":"2024-11-14T03:39:15.329146Z","iopub.execute_input":"2024-11-14T03:39:15.329899Z","iopub.status.idle":"2024-11-14T03:39:17.221692Z","shell.execute_reply.started":"2024-11-14T03:39:15.329860Z","shell.execute_reply":"2024-11-14T03:39:17.220918Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"def format_context(sample):\n    sample['context'] = prompt_template\n    return sample","metadata":{"execution":{"iopub.status.busy":"2024-11-14T03:42:43.186034Z","iopub.execute_input":"2024-11-14T03:42:43.186716Z","iopub.status.idle":"2024-11-14T03:42:43.191268Z","shell.execute_reply.started":"2024-11-14T03:42:43.186671Z","shell.execute_reply":"2024-11-14T03:42:43.190131Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"dataset_train2_2 = dataset_train2.map(format_context, batched=False)\ndataset_val2_2 = dataset_val2.map(format_context, batched=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T03:42:58.058687Z","iopub.execute_input":"2024-11-14T03:42:58.059476Z","iopub.status.idle":"2024-11-14T03:42:58.181709Z","shell.execute_reply.started":"2024-11-14T03:42:58.059438Z","shell.execute_reply":"2024-11-14T03:42:58.180746Z"},"trusted":true},"execution_count":81,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1132 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"398008b3d42d44d6a185fd0621ab523b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/126 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a927dfc018948cd8c5160689c5a9979"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_train2_2['question'][1]","metadata":{"execution":{"iopub.status.busy":"2024-11-14T03:43:48.361600Z","iopub.execute_input":"2024-11-14T03:43:48.362439Z","iopub.status.idle":"2024-11-14T03:43:48.371371Z","shell.execute_reply.started":"2024-11-14T03:43:48.362396Z","shell.execute_reply":"2024-11-14T03:43:48.370245Z"},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"\"Danh sách các cải tiến có tác giả là 'Tran Thi H' và số công việc áp dụng từ 10 đến 25.\""},"metadata":{}}]},{"cell_type":"code","source":"dataset['train'][0]['context']","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:55:55.931487Z","iopub.execute_input":"2024-11-14T08:55:55.932139Z","iopub.status.idle":"2024-11-14T08:55:55.938311Z","shell.execute_reply.started":"2024-11-14T08:55:55.932101Z","shell.execute_reply":"2024-11-14T08:55:55.937414Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"' \\nYou are an SQL query assistant. Based on the table information below, generate an SQL query to retrieve the relevant information for the user. If the user’s question is unrelated to the table, respond naturally in user\\'s language.\\n\\nThe jidouka table contains the following columns:\\n\\nid: Row identifier (int)\\ntên_cải_tiến: Name of the improvement (str)\\nloại_hình_công_việc: Type of work that the improvement is intended to enhance (str) (e.g., database processing, data entry, workflow optimization, etc.)\\ncông_cụ: Tool used to achieve the improvement (str) (e.g., Python, Excel, Visual Studio Code, etc.)\\nmô_tả: Detailed description of the improvement (str) (e.g., each step of the improvement process)\\nsản_phẩm: Output product of the improvement (str) (e.g., .csv file, .xlsx file, etc.)\\ntác_giả: Contributor, company employee, or creator of the improvement (str)\\nbộ_phận: Department of the author, usually referred to as \"dc\" (str) (e.g., dc1, dc2, dc3, dcd, souko, etc.)\\nsố_giờ: Number of hours saved by applying the improvement (int)\\nsố_công_việc_áp_dụng: Number of tasks in the company that the improvement has supported (int)\\nthời_điểm_ra_mắt: Launch date of the tool (str) (e.g., 2024-10-11, 2024-10-09, etc.)\\nthông_tin_thêm: Link to additional documentation (PowerPoint, video) on using the improvement or the improvement’s tool (str)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"def format_data_template(sample):\n    chat = [\n          {\"role\":\"system\", \"content\": sample['context']},\n          {\"role\":\"user\", \"content\":sample['question']},\n          {\"role\":\"assistant\",\"content\":sample['answer']}\n    ]\n    return {\n        \"messages\": tokenizer.apply_chat_template(chat, tokenize=False)\n    }","metadata":{"execution":{"iopub.status.busy":"2024-11-14T05:04:44.108812Z","iopub.execute_input":"2024-11-14T05:04:44.109723Z","iopub.status.idle":"2024-11-14T05:04:44.115844Z","shell.execute_reply.started":"2024-11-14T05:04:44.109681Z","shell.execute_reply":"2024-11-14T05:04:44.114665Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def format_data_template_to_token(sample):\n    chat = [\n          {\"role\":\"system\", \"content\": sample['context']},\n          {\"role\":\"user\", \"content\":sample['question']}\n    ]\n    sample['input_ids'] = tokenizer.apply_chat_template(chat, tokenize=True, padding=True, truncation=True, return_tensors='pt')\n    sample['labels'] = tokenizer(sample['answer'], padding=True, truncation=True, return_tensors='pt').input_ids\n    \n    return sample\ntokenized_dataset_train = dataset['train'].map(format_data_template_to_token, remove_columns=['context','question','answer'], batched=True)\ntokenized_dataset_valid = dataset['validation'].map(format_data_template_to_token, remove_columns=['context','question', 'answer'], batched=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = dataset['train'].map(format_data_template, remove_columns=['context','question','answer'])","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:56:05.722197Z","iopub.execute_input":"2024-11-14T08:56:05.723099Z","iopub.status.idle":"2024-11-14T08:56:05.943184Z","shell.execute_reply.started":"2024-11-14T08:56:05.723058Z","shell.execute_reply":"2024-11-14T08:56:05.942311Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"dataset_valid = dataset['validation'].map(format_data_template, remove_columns=['context', 'question','answer'])","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:56:03.779972Z","iopub.execute_input":"2024-11-14T08:56:03.780853Z","iopub.status.idle":"2024-11-14T08:56:03.989581Z","shell.execute_reply.started":"2024-11-14T08:56:03.780813Z","shell.execute_reply":"2024-11-14T08:56:03.988529Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"dataset_train['messages'][0]","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:56:08.706215Z","iopub.execute_input":"2024-11-14T08:56:08.706588Z","iopub.status.idle":"2024-11-14T08:56:08.717223Z","shell.execute_reply.started":"2024-11-14T08:56:08.706555Z","shell.execute_reply":"2024-11-14T08:56:08.716416Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 14 Nov 2024\\n\\nYou are an SQL query assistant. Based on the table information below, generate an SQL query to retrieve the relevant information for the user. If the user’s question is unrelated to the table, respond naturally in user\\'s language.\\n\\nThe jidouka table contains the following columns:\\n\\nid: Row identifier (int)\\ntên_cải_tiến: Name of the improvement (str)\\nloại_hình_công_việc: Type of work that the improvement is intended to enhance (str) (e.g., database processing, data entry, workflow optimization, etc.)\\ncông_cụ: Tool used to achieve the improvement (str) (e.g., Python, Excel, Visual Studio Code, etc.)\\nmô_tả: Detailed description of the improvement (str) (e.g., each step of the improvement process)\\nsản_phẩm: Output product of the improvement (str) (e.g., .csv file, .xlsx file, etc.)\\ntác_giả: Contributor, company employee, or creator of the improvement (str)\\nbộ_phận: Department of the author, usually referred to as \"dc\" (str) (e.g., dc1, dc2, dc3, dcd, souko, etc.)\\nsố_giờ: Number of hours saved by applying the improvement (int)\\nsố_công_việc_áp_dụng: Number of tasks in the company that the improvement has supported (int)\\nthời_điểm_ra_mắt: Launch date of the tool (str) (e.g., 2024-10-11, 2024-10-09, etc.)\\nthông_tin_thêm: Link to additional documentation (PowerPoint, video) on using the improvement or the improvement’s tool (str)<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat improvements were launched on October 11, 2024?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSELECT * FROM jidouka WHERE thời_điểm_ra_mắt LIKE LOWER(\\'%2024-10-11%\\');<|eot_id|>'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=10,\n    eval_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:56:30.834597Z","iopub.execute_input":"2024-11-14T08:56:30.835354Z","iopub.status.idle":"2024-11-14T08:56:30.869038Z","shell.execute_reply.started":"2024-11-14T08:56:30.835309Z","shell.execute_reply":"2024-11-14T08:56:30.868292Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset_train,\n    eval_dataset=dataset_valid,\n    peft_config=peft_config,\n    max_seq_length=512,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    dataset_text_field='messages',\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:56:34.163226Z","iopub.execute_input":"2024-11-14T08:56:34.164104Z","iopub.status.idle":"2024-11-14T08:56:34.719471Z","shell.execute_reply.started":"2024-11-14T08:56:34.164061Z","shell.execute_reply":"2024-11-14T08:56:34.718711Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/104 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"934c8ded590a47ca9aeb1998050898b3"}},"metadata":{}}]},{"cell_type":"code","source":"eot = \"<|eot_id|>\"\neot_id = tokenizer.convert_tokens_to_ids(eot)\ntokenizer.pad_token = eot\ntokenizer.pad_token_id = eot_id","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:56:39.365191Z","iopub.execute_input":"2024-11-14T08:56:39.366029Z","iopub.status.idle":"2024-11-14T08:56:39.370609Z","shell.execute_reply.started":"2024-11-14T08:56:39.365987Z","shell.execute_reply":"2024-11-14T08:56:39.369336Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:56:46.345809Z","iopub.execute_input":"2024-11-14T08:56:46.346178Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241114_085647-zjzk89rp</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/huyhoangt2201-fpt-university/huggingface/runs/zjzk89rp' target=\"_blank\">llama-3.2-1b-sql_finetuned_billingual_3.0</a></strong> to <a href='https://wandb.ai/huyhoangt2201-fpt-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/huyhoangt2201-fpt-university/huggingface' target=\"_blank\">https://wandb.ai/huyhoangt2201-fpt-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/huyhoangt2201-fpt-university/huggingface/runs/zjzk89rp' target=\"_blank\">https://wandb.ai/huyhoangt2201-fpt-university/huggingface/runs/zjzk89rp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='149' max='4680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 149/4680 03:56 < 2:01:38, 0.62 it/s, Epoch 0.32/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = 'llama-3.2-1b-sql_finetuned_billingual_3.0_adapter'\nnew_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = 'huyhoangt2201/llama-3.2-1b-sql_finetuned_billingual_3.0_adapter'\nbase_model = 'phamhai/Llama-3.2-1B-Instruct-Frog'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model)\n\nbase_model_reload = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        return_dict=True,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\n\n# base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n\n# Merge adapter with base model\nmerge_model = PeftModel.from_pretrained(base_model_reload, new_model)\n\nmerge_model = merge_model.merge_and_unload()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model_merged = 'llama-3.2-1b-sql_finetuned_billingual_3.0_merged'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merge_model.save_pretrained(new_model_merged)\ntokenizer.save_pretrained(new_model_merged)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token = hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T03:09:06.932328Z","iopub.execute_input":"2024-11-14T03:09:06.932768Z","iopub.status.idle":"2024-11-14T03:09:07.163913Z","shell.execute_reply.started":"2024-11-14T03:09:06.932725Z","shell.execute_reply":"2024-11-14T03:09:07.162803Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"merge_model.push_to_hub(new_model_merged, use_temp_dir=False)\ntokenizer.push_to_hub(new_model_merged, use_temp_dir=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## new_model inference","metadata":{}},{"cell_type":"code","source":"new_model_name = 'huyhoangt2201/llama-3.2-1b-sql_finetuned_billingual_2.0_merged'","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:46:41.120277Z","iopub.execute_input":"2024-11-14T08:46:41.120685Z","iopub.status.idle":"2024-11-14T08:46:41.125584Z","shell.execute_reply.started":"2024-11-14T08:46:41.120650Z","shell.execute_reply":"2024-11-14T08:46:41.124566Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\ndevice = torch.device('cuda')\nmodel_path = new_model_name\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path).to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:46:45.698419Z","iopub.execute_input":"2024-11-14T08:46:45.698799Z","iopub.status.idle":"2024-11-14T08:47:50.108396Z","shell.execute_reply.started":"2024-11-14T08:46:45.698763Z","shell.execute_reply":"2024-11-14T08:47:50.107407Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1837451d5a1249debb57ad01abaa4de6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34a94101082a4c7bac38b94dedb2d228"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3d46abfa515430f816bc97074a1bd28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/928 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be5fef82c87a4663b61710a9ac91f605"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"211b116131bf499987a639a1c75abe2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cec082bbdcf4119b678f469fe05f014"}},"metadata":{}}]},{"cell_type":"code","source":"prompt_template = \"\"\"\nYou are an SQL query assistant. Based on the table information below, generate an SQL query to retrieve the relevant information for the user. If the user’s question is unrelated to the table, respond naturally in human language.\n\nThe jidouka table contains the following columns:\nid: Row identifier (int)\ntên_cải_tiến: Name of the improvement (str)\nloại_hình_công_việc: Type of work that the improvement is intended to enhance (str) (e.g., database processing, data entry, workflow optimization, etc.)\ncông_cụ: Tool used to achieve the improvement (str) (e.g., Python, Excel, Visual Studio Code, etc.)\nmô_tả: Detailed description of the improvement (str) (e.g., each step of the improvement process)\nsản_phẩm: Output product of the improvement (str) (e.g., .csv file, .xlsx file, etc.)\ntác_giả: Contributor, company employee, or creator of the improvement (str)\nbộ_phận: Department of the author, usually referred to as \"dc\" (str) (e.g., dc1, dc2, dc3, dcd, souko, etc.)\nsố_giờ: Number of hours saved by applying the improvement (int)\nsố_công_việc_áp_dụng: Number of tasks in the company that the improvement has supported (int)\nthời_điểm_ra_mắt: Launch date of the tool (str) (e.g., 2024-10-11, 2024-10-09, etc.)\nthông_tin_thêm: Link to additional documentation (PowerPoint, video) on using the improvement or the improvement’s tool (str)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:47:53.911749Z","iopub.execute_input":"2024-11-14T08:47:53.912137Z","iopub.status.idle":"2024-11-14T08:47:53.918147Z","shell.execute_reply.started":"2024-11-14T08:47:53.912100Z","shell.execute_reply":"2024-11-14T08:47:53.916971Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from typing import List, Dict\nclass ContextAwareChatbot:\n    def __init__(self,prompt, max_history: int = 5):\n        self.model = model\n        self.tokenizer = tokenizer \n        self.max_history = max_history\n        self.conversation_history: List[Dict[str, str]] = []\n        self.prompt=prompt\n    def _build_prompt(self) -> str:\n        # Build context from history\n\n        return self.prompt\n\n    def _clean_response(self, response: str) -> str:\n        # Clean up the generated response\n        response = response.split(\"Assistant:\")[-1].strip()\n        # Stop at any new \"Human:\" or \"Assistant:\" markers\n        if \"Human:\" in response:\n            response = response.split(\"Human:\")[0].strip()\n        return response\n\n    def chat(self, user_input: str) -> str:\n        # Generate the contextualized prompt\n        prompt = self._build_prompt()\n\n#         # Generate response\n#         response = self.pipeline(\n#             prompt,\n#             return_full_text=False,\n#             clean_up_tokenization_spaces=True\n#         )[0]['generated_text']\n\n#         # Clean the response\n#         cleaned_response = self._clean_response(response)\n        messages =[\n            {'role':'system',\n             'content':prompt}\n            ,\n            {'role':'user',\n             'content':user_input}\n        ]\n        tokenized_chat = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors='pt').to('cuda')\n        outputs = self.model.generate(tokenized_chat, max_new_tokens=256).to('cuda')\n        bot_response = self.tokenizer.decode(outputs[0])\n        bot_response = bot_response.split('<|start_header_id|>assistant<|end_header_id|>')\n        bot_response = bot_response[1].strip()[:-10]\n        # Update conversation history\n        self.conversation_history.append({\n            'human': user_input,\n            'assistant': bot_response\n        })\n\n        return bot_response\n\n    def get_history(self) -> List[Dict[str, str]]:\n        return self.conversation_history\n\n    def clear_history(self):\n        self.conversation_history = []\n\n# 4. Create chatbot instance\nchatbot = ContextAwareChatbot(prompt_template)\n\n# 5. Example usage function\ndef chat_session():\n    print(\"Chatbot initialized. Type 'exit' to end the conversation, 'clear' to clear history.\")\n\n    while True:\n        user_input = input(\"\\nYou: \").strip()\n\n        if user_input.lower() == 'exit':\n            print(\"Goodbye!\")\n            break\n        elif user_input.lower() == 'clear':\n            chatbot.clear_history()\n            print(\"Conversation history cleared!\")\n            continue\n\n        response = chatbot.chat(user_input)\n        print(f\"\\nAssistant: {response}\")\n\n# 6. Example of how to use\nif __name__ == \"__main__\":\n    chat_session()","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:47:59.969801Z","iopub.execute_input":"2024-11-14T08:47:59.970176Z","iopub.status.idle":"2024-11-14T08:52:29.918031Z","shell.execute_reply.started":"2024-11-14T08:47:59.970141Z","shell.execute_reply":"2024-11-14T08:52:29.916650Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Chatbot initialized. Type 'exit' to end the conversation, 'clear' to clear history.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nYou:  xin chào\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: Danh sách các cải tiến có số giờ tiết kiệm trên 5 và công cụ hỗ trợ là Excel? WHERE số_giờ > 5 AND công_cụ LIKE LOWER('%Excel%'); SELECT * FROM jidouka WHERE số_giờ > 5 AND công_cụ LIKE LOWER('%Excel%'); SELECT * FROM jidouka WHERE số_giờ > 5 AND công_cụ LIKE LOWER('%Excel%'); SELECT tên_cải_tiến FROM jidouka WHERE số_giờ > 5 AND công_cụ LIKE LOWER('%Excel%'); SELECT tên_cải_tiến FROM jidouka WHERE số_giờ > 5 AND công_cụ LIKE LOWER('%Excel%'); SELECT tên_cải_tiến FROM jidouka WHERE số_giờ > 5 AND công_cụ LIKE LOWER('%Excel%'); SELECT tên_cải_tiến FROM jidouka WHERE số_giờ > 5 AND công_cụ LIKE LOWER('%Excel%'); SELECT tên_cải_tiến FROM jidouka WHERE số_giờ > 5 AND công_cụ LIKE LOWER('%Excel%'); SELECT tên_cải_tiến FROM jidouka WHERE \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nYou:  How many contributors have worked on improvements?\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: SELECT COUNT(DISTINCT tác_giả) FROM jidouka;\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nYou:  List all improvements launched on a Friday in 2024.\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: SELECT * FROM jidouka WHERE thời_điểm_ra_mắt IN (SELECT thời_điểm_ra_mắt FROM jidouka WHERE DAYOFWEEK(thời_điểm_ra_mắt) = 6); (str) (e.g., 2024-04-10, 2024-04-19, etc.)\nSELECT thời_điểm_ra_mắt FROM jidouka WHERE thời_điểm_ra_mắt IN (SELECT thời_điểm_ra_mắt FROM jidouka WHERE DAYOFWEAR = '2024-04-10'); (str) (e.g., 2024-04-10, 2024-04-09, etc.)\nSELECT thời_điểm_ra_mắt FROM jidouka WHERE thời_điểm_ra_mắt IN (SELECT thời_điểm_ra_mắt FROM jidouka WHERE thời_điểm_ra_mô_tả LIKE LOWER('%2024-04-10%'); (str) (e.g., 2024-04-10, 2024-04-09, etc.)\nSELECT thời_điểm_ra_mắt FROM jidouka WHERE thời_điểm_ra_mô_tả LIK\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nYou:  Find improvements from dcd related to workflow optimization.\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: SELECT * FROM jidouka WHERE bộ_phận LIKE LOWER('%dcd%') AND loại_hình_công_việc LIKE LOWER('%workflow optimization%'); SELECT * FROM jidouka WHERE bộ_phận LIKE LOWER('%dcd%') AND loại_hình_công_việc LIKE LOWER('%workflow optimization%'); SELECT * FROM jidouka WHERE bộ_phận LIKE LOWER('%dcd%') AND loại_hình_công_việc LIKE LOWER('%workflow optimization%'); SELECT * FROM jidouka WHERE bộ_phận LIKE LOWER('%dcd%') AND loại_hình_công_việc LIKE LOWER('%workflow optimization%'); SELECT * FROM jidouka WHERE bộ_phận LIKE LOWER('%dcd%') AND loại_hình_công_việc LIKE LOWER('%workflow optimization%'); SELECT * FROM jidouka WHERE bộ_phận LIKE LOWER('%dcd%') AND loại_hình_công_việc LIKE LOWER('%workflow optimization%'); SELECT * FROM jidouka WHERE bộ_phận LIKE LOWER('%dcd%') AND loại_hình_công_việc LIKE LOWER('%workflow optimization%'); SELECT * FROM jidouka WHERE bộ_phận LIKE LOWE\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nYou:   relevant information for the user. If the user’s question is unrelated to the table, respond naturally in user's language. The jidouka table contains the following columns: id: Row identifier (int) tên_cải_tiến: Name of the improvement (str) loại_hình_công_việc: Type of work that the improvement is intended to enhance (str) (e.g., database processing, data entry, workflow optimization, etc.) công_cụ: Tool used to achieve the improvement (str) (e.g., Python, Excel, Visual Studio Code, etc.) mô_tả: Detailed description of the improvement (str) (e.g., each step of the improvement process) sản_phẩm: Output product of the improvement (str) (e.g., .csv file, .xlsx file, etc.) tác_giả: Contributor, company employee, or creator of the improvement (str) bộ_phận: Department of the author, usually referred to as \"dc\" (str) (e.g., dc1, dc2, dc3, dcd, souko, etc.) số_giờ: Number of hours saved by applying the improvement (int) số_công_việc_áp_dụng: Number of tasks in the company that the improvement has supported (int) thời_điểm_ra_mắt: Launch date of the tool (str) (e.g., 2024-10-11, 2024-10-09, etc.) thông_tin_thêm: Link to additional documentation (PowerPoint, video) on using the improvement or the improvement’s tool (str) Excel có những công dụng gì?\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: Liệt kê các cải tiến có sản phẩm đầu ra là file txt và tiết kiệm ít nhất 4 giờ. Đụ ngôn nào có sản phẩm đầu ra là file txt và tiết kiệm ít nhất 4 giờ? NULL\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nYou:  Excel có những công dụng gì?\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: Excel có số công việc áp dụng lớn hơn 3. Nó có công cụ hỗ trợ là Python và có số công việc áp dụng trên 6.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nYou:  Tìm những cải tiến có sử dụng nhiều công cụ nhất (số lượng dấu phẩy trong cột công_cụ nhiều nhất)\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: SELECT tên_cải_tiến FROM jidouka WHERE công_cụ ORDER BY số_giờ DESC LIMIT 1; SELECT tên_cải_tiến FROM jidouka WHERE công_cụ LIKE LOWER('%tổng số giờ tiết kiệm của các cải tiến có công cụ hỗ trợ là Excel%'); SELECT tên_cải_tiến FROM jidouka WHERE công_cụ LIKE LOWER('%Excel%') ORDER BY số_giờ DESC LIMIT 1; SELECT tên_cải_tiến FROM jidouka WHERE công_cụ LIKE LOWER('%Excel%') ORDER BY số_giờ DESC LIMIT 1; SELECT tên_cải_tiến FROM jidouka WHERE công_cụ LIKE LOWER('%Excel%') ORDER BY số_giờ DESC LIMIT 1; SELECT tên_cải_tiến FROM jidouka WHERE công_cụ LIKE LOWER('%Excel%') ORDER BY số_giờ DESC LIMIT 1; SELECT tên_cải_tiến FROM jidouka WHERE công_cụ LIKE LOWER('%Excel%') ORDER BY số_giờ DESC LIMIT 1; SELECT tên_cải_tiến FROM jidouka WHERE công_cụ LIKE LOWER('%Excel%') ORDER BY số_giờ DESC LIMIT\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# 6. Example of how to use\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[43mchat_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[31], line 69\u001b[0m, in \u001b[0;36mchat_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot initialized. Type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to end the conversation, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to clear history.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"],"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error"}]}]}