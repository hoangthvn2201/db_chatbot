{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch  --quiet\n\n# # Install Hugging Face libraries\n!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n\n# #FlashAttention only supports Ampere GPUs or newer. #NEED A100 IN GOOGLE COLAB\n!pip install -U transformers\n# # !pip install -U flash-attn --no-build-isolation --quiet\n\n\n! pip install peft --quiet\n! pip install datasets trl ninja packaging --quiet\n\n# # Uncomment only if you're using A100 GPU\n# #!pip install flash-attn --no-build-isolation\n!pip install diffusers safetensors  --quiet\n\n# %pip install -U wandb","metadata":{"execution":{"iopub.status.busy":"2024-11-19T08:58:37.082513Z","iopub.execute_input":"2024-11-19T08:58:37.082869Z","iopub.status.idle":"2024-11-19T08:59:53.479401Z","shell.execute_reply.started":"2024-11-19T08:58:37.082839Z","shell.execute_reply":"2024-11-19T08:59:53.478245Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset, DatasetDict\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-11-19T08:59:53.481276Z","iopub.execute_input":"2024-11-19T08:59:53.481631Z","iopub.status.idle":"2024-11-19T09:00:28.195868Z","shell.execute_reply.started":"2024-11-19T08:59:53.481591Z","shell.execute_reply":"2024-11-19T09:00:28.195170Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\ndevice = torch.device('cuda'if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:00:28.196876Z","iopub.execute_input":"2024-11-19T09:00:28.197417Z","iopub.status.idle":"2024-11-19T09:00:28.250131Z","shell.execute_reply.started":"2024-11-19T09:00:28.197389Z","shell.execute_reply":"2024-11-19T09:00:28.249048Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on SQL dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:13:06.770595Z","iopub.execute_input":"2024-11-19T09:13:06.770954Z","iopub.status.idle":"2024-11-19T09:13:12.845639Z","shell.execute_reply.started":"2024-11-19T09:13:06.770925Z","shell.execute_reply":"2024-11-19T09:13:12.844870Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuyhoangt2201\u001b[0m (\u001b[33mhuyhoangt2201-fpt-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113383066668172, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f17bbd50599430fb7d4934fb4de2153"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241119_091308-fh7onmte</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset/runs/fh7onmte' target=\"_blank\">glorious-pine-11</a></strong> to <a href='https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset' target=\"_blank\">https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset/runs/fh7onmte' target=\"_blank\">https://wandb.ai/huyhoangt2201-fpt-university/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset/runs/fh7onmte</a>"},"metadata":{}}]},{"cell_type":"code","source":"base_model = \"phamhai/Llama-3.2-1B-Instruct-Frog\"\nnew_model = \"llama-3.2-1b-sql_finetuned_multitableJidouka_1.0\"","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:13:18.276908Z","iopub.execute_input":"2024-11-19T09:13:18.277245Z","iopub.status.idle":"2024-11-19T09:13:18.282177Z","shell.execute_reply.started":"2024-11-19T09:13:18.277209Z","shell.execute_reply":"2024-11-19T09:13:18.281357Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"torch_dtype = torch.float16\n\n# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float32\n    #attn_implementation=attn_implementation\n)\ntokenizer = AutoTokenizer.from_pretrained(base_model,use_fast=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:13:21.333210Z","iopub.execute_input":"2024-11-19T09:13:21.334003Z","iopub.status.idle":"2024-11-19T09:14:26.740885Z","shell.execute_reply.started":"2024-11-19T09:13:21.333968Z","shell.execute_reply":"2024-11-19T09:14:26.740236Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7db24c23103544f3b5d6b85e2d276f57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fe72df15a7548529dc9f9c2cf3d1f25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17170793166d4fd0b4c9dcc2a6f1fade"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ab62f98be90430dbfef08554a5534b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e69b6840d171439db192fb26734e8d13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d4e1dae1cac4ffa9b267f12d8dd77fc"}},"metadata":{}}]},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:14:31.832053Z","iopub.execute_input":"2024-11-19T09:14:31.832401Z","iopub.status.idle":"2024-11-19T09:14:32.087700Z","shell.execute_reply.started":"2024-11-19T09:14:31.832370Z","shell.execute_reply":"2024-11-19T09:14:32.086810Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"dataset_train = load_dataset(\"huyhoangt2201/multitableJidouka-1.1-english-only-dataset\", split='train[:90%]')\ndataset_val = load_dataset(\"huyhoangt2201/multitableJidouka-1.1-english-only-dataset\", split='train[-10%:]')\ndataset = DatasetDict({\n    'train': dataset_train,\n    'validation': dataset_val\n})\ndataset.save_to_disk(\"completed_train_dataset\")","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:14:35.758450Z","iopub.execute_input":"2024-11-19T09:14:35.758839Z","iopub.status.idle":"2024-11-19T09:14:37.894441Z","shell.execute_reply.started":"2024-11-19T09:14:35.758807Z","shell.execute_reply":"2024-11-19T09:14:37.893668Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"(…)07_records_english_multitableJidouka.csv:   0%|          | 0.00/1.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f37efe5b6fd451982b814e7beb4deea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/507 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5933b486d7c04556ba808a4c9ada14bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/456 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07945d25f6bd4387a806bc436cf39e0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/51 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d8f1d9736094c47929024ecc26f088c"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_train2 = load_dataset(\"huyhoangt2201/jidouka3.1\", split='train[:90%]')\ndataset_val2 = load_dataset(\"huyhoangt2201/jidouka3.1\", split='train[-10%:]')\ndataset2 = DatasetDict({\n    'train': dataset_train2,\n    'validation': dataset_val2\n})","metadata":{"execution":{"iopub.status.busy":"2024-11-14T03:39:15.329146Z","iopub.execute_input":"2024-11-14T03:39:15.329899Z","iopub.status.idle":"2024-11-14T03:39:17.221692Z","shell.execute_reply.started":"2024-11-14T03:39:15.329860Z","shell.execute_reply":"2024-11-14T03:39:17.220918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_context(sample):\n    sample['context'] = prompt_template\n    return sample","metadata":{"execution":{"iopub.status.busy":"2024-11-14T03:42:43.186034Z","iopub.execute_input":"2024-11-14T03:42:43.186716Z","iopub.status.idle":"2024-11-14T03:42:43.191268Z","shell.execute_reply.started":"2024-11-14T03:42:43.186671Z","shell.execute_reply":"2024-11-14T03:42:43.190131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train2_2 = dataset_train2.map(format_context, batched=False)\ndataset_val2_2 = dataset_val2.map(format_context, batched=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T03:42:58.058687Z","iopub.execute_input":"2024-11-14T03:42:58.059476Z","iopub.status.idle":"2024-11-14T03:42:58.181709Z","shell.execute_reply.started":"2024-11-14T03:42:58.059438Z","shell.execute_reply":"2024-11-14T03:42:58.180746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train2_2['question'][1]","metadata":{"execution":{"iopub.status.busy":"2024-11-14T03:43:48.361600Z","iopub.execute_input":"2024-11-14T03:43:48.362439Z","iopub.status.idle":"2024-11-14T03:43:48.371371Z","shell.execute_reply.started":"2024-11-14T03:43:48.362396Z","shell.execute_reply":"2024-11-14T03:43:48.370245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['train'][0]['context']","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:14:47.238209Z","iopub.execute_input":"2024-11-19T09:14:47.238566Z","iopub.status.idle":"2024-11-19T09:14:47.250363Z","shell.execute_reply.started":"2024-11-19T09:14:47.238534Z","shell.execute_reply":"2024-11-19T09:14:47.249470Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"\" \\nYou are an SQL query assistant. Based on schema below, generate an SQL query to retrieve the relevant information for the user. If the user’s question is unrelated to the table, respond naturally in user's language.\\n\\nSchema:\\n-- Table: Job\\nCREATE TABLE Job (\\n    Id INT PRIMARY KEY AUTO_INCREMENT,\\n    Job_name NVARCHAR(255) NOT NULL,\\n    DateCreate DATETIME NOT NULL,\\n    ModifiedDate DATETIME,\\n    CreateBy NVARCHAR(255),\\n    ModifiedBy NVARCHAR(255)\\n);\\n\\n-- Table: Department\\nCREATE TABLE Department (\\n    Id INT PRIMARY KEY AUTO_INCREMENT,\\n    Department_name NVARCHAR(255) NOT NULL,\\n    DateCreate DATETIME NOT NULL,\\n    ModifiedDate DATETIME,\\n    CreateBy NVARCHAR(255),\\n    ModifiedBy NVARCHAR(255)\\n);\\n\\n-- Table: Author\\nCREATE TABLE Author (\\n    Id INT PRIMARY KEY AUTO_INCREMENT,\\n    Author_name NVARCHAR(255) NOT NULL,\\n    DateCreate DATETIME NOT NULL,\\n    ModifiedDate DATETIME,\\n    CreateBy NVARCHAR(255),\\n    ModifiedBy NVARCHAR(255)\\n);\\n\\n-- Table: Tool\\nCREATE TABLE Tool (\\n    Id INT PRIMARY KEY AUTO_INCREMENT,\\n    Tool_name NVARCHAR(255) NOT NULL,\\n    DateCreate DATETIME NOT NULL,\\n    ModifiedDate DATETIME,\\n    CreateBy NVARCHAR(255),\\n    ModifiedBy NVARCHAR(255)\\n);\\n\\n-- Table: Jidouka\\nCREATE TABLE Jidouka (\\n    Id BIGINT PRIMARY KEY AUTO_INCREMENT,\\n    Improve_name NVARCHAR(255) NOT NULL,\\n    Job_id INT,\\n    Department_id INT,\\n    Author_id INT,\\n    Description NVARCHAR(255),\\n    Product_name NVARCHAR(255),\\n    Time INT,\\n    Applications INT,\\n    Release_date DATETIME,\\n    Other_info NVARCHAR(255),\\n    DateCreate DATETIME NOT NULL,\\n    ModifiedDate DATETIME,\\n    CreateBy NVARCHAR(255),\\n    ModifiedBy NVARCHAR(255),\\n    FOREIGN KEY (Job_id) REFERENCES Job(Id),\\n    FOREIGN KEY (Department_id) REFERENCES Department(Id),\\n    FOREIGN KEY (Author_id) REFERENCES Author(Id)\\n);\\n\\n-- Table: JidoukaTool\\nCREATE TABLE JidoukaTool (\\n    Jidouka_id BIGINT,\\n    Tool_id INT,\\n    PRIMARY KEY (Jidouka_id, Tool_id),\\n    FOREIGN KEY (Jidouka_id) REFERENCES Jidouka(Id),\\n    FOREIGN KEY (Tool_id) REFERENCES Tool(Id)\\n);\\n\""},"metadata":{}}]},{"cell_type":"code","source":"def format_data_template(sample):\n    chat = [\n          {\"role\":\"system\", \"content\": sample['context']},\n          {\"role\":\"user\", \"content\":sample['question']},\n          {\"role\":\"assistant\",\"content\":sample['answer']}\n    ]\n    return {\n        \"messages\": tokenizer.apply_chat_template(chat, tokenize=False)\n    }","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:14:58.133381Z","iopub.execute_input":"2024-11-19T09:14:58.133759Z","iopub.status.idle":"2024-11-19T09:14:58.139666Z","shell.execute_reply.started":"2024-11-19T09:14:58.133728Z","shell.execute_reply":"2024-11-19T09:14:58.138938Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def format_data_template_to_token(sample):\n    chat = [\n          {\"role\":\"system\", \"content\": sample['context']},\n          {\"role\":\"user\", \"content\":sample['question']}\n    ]\n    sample['input_ids'] = tokenizer.apply_chat_template(chat, tokenize=True, padding=True, truncation=True, return_tensors='pt')\n    sample['labels'] = tokenizer(sample['answer'], padding=True, truncation=True, return_tensors='pt').input_ids\n    \n    return sample\ntokenized_dataset_train = dataset['train'].map(format_data_template_to_token, remove_columns=['context','question','answer'], batched=True)\ntokenized_dataset_valid = dataset['validation'].map(format_data_template_to_token, remove_columns=['context','question', 'answer'], batched=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = dataset['train'].map(format_data_template, remove_columns=['context','question','answer'])","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:15:01.053306Z","iopub.execute_input":"2024-11-19T09:15:01.054004Z","iopub.status.idle":"2024-11-19T09:15:01.317631Z","shell.execute_reply.started":"2024-11-19T09:15:01.053969Z","shell.execute_reply":"2024-11-19T09:15:01.316747Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/456 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2b263c1be3c4015a0db1f6b5cf0332c"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_valid = dataset['validation'].map(format_data_template, remove_columns=['context', 'question','answer'])","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:15:03.133098Z","iopub.execute_input":"2024-11-19T09:15:03.133926Z","iopub.status.idle":"2024-11-19T09:15:03.321747Z","shell.execute_reply.started":"2024-11-19T09:15:03.133891Z","shell.execute_reply":"2024-11-19T09:15:03.320799Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/51 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab96cf789619465c8902de6f0f5115f9"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_train['messages'][0]","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:15:05.198155Z","iopub.execute_input":"2024-11-19T09:15:05.198546Z","iopub.status.idle":"2024-11-19T09:15:05.207038Z","shell.execute_reply.started":"2024-11-19T09:15:05.198512Z","shell.execute_reply":"2024-11-19T09:15:05.206134Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 19 Nov 2024\\n\\nYou are an SQL query assistant. Based on schema below, generate an SQL query to retrieve the relevant information for the user. If the user’s question is unrelated to the table, respond naturally in user's language.\\n\\nSchema:\\n-- Table: Job\\nCREATE TABLE Job (\\n    Id INT PRIMARY KEY AUTO_INCREMENT,\\n    Job_name NVARCHAR(255) NOT NULL,\\n    DateCreate DATETIME NOT NULL,\\n    ModifiedDate DATETIME,\\n    CreateBy NVARCHAR(255),\\n    ModifiedBy NVARCHAR(255)\\n);\\n\\n-- Table: Department\\nCREATE TABLE Department (\\n    Id INT PRIMARY KEY AUTO_INCREMENT,\\n    Department_name NVARCHAR(255) NOT NULL,\\n    DateCreate DATETIME NOT NULL,\\n    ModifiedDate DATETIME,\\n    CreateBy NVARCHAR(255),\\n    ModifiedBy NVARCHAR(255)\\n);\\n\\n-- Table: Author\\nCREATE TABLE Author (\\n    Id INT PRIMARY KEY AUTO_INCREMENT,\\n    Author_name NVARCHAR(255) NOT NULL,\\n    DateCreate DATETIME NOT NULL,\\n    ModifiedDate DATETIME,\\n    CreateBy NVARCHAR(255),\\n    ModifiedBy NVARCHAR(255)\\n);\\n\\n-- Table: Tool\\nCREATE TABLE Tool (\\n    Id INT PRIMARY KEY AUTO_INCREMENT,\\n    Tool_name NVARCHAR(255) NOT NULL,\\n    DateCreate DATETIME NOT NULL,\\n    ModifiedDate DATETIME,\\n    CreateBy NVARCHAR(255),\\n    ModifiedBy NVARCHAR(255)\\n);\\n\\n-- Table: Jidouka\\nCREATE TABLE Jidouka (\\n    Id BIGINT PRIMARY KEY AUTO_INCREMENT,\\n    Improve_name NVARCHAR(255) NOT NULL,\\n    Job_id INT,\\n    Department_id INT,\\n    Author_id INT,\\n    Description NVARCHAR(255),\\n    Product_name NVARCHAR(255),\\n    Time INT,\\n    Applications INT,\\n    Release_date DATETIME,\\n    Other_info NVARCHAR(255),\\n    DateCreate DATETIME NOT NULL,\\n    ModifiedDate DATETIME,\\n    CreateBy NVARCHAR(255),\\n    ModifiedBy NVARCHAR(255),\\n    FOREIGN KEY (Job_id) REFERENCES Job(Id),\\n    FOREIGN KEY (Department_id) REFERENCES Department(Id),\\n    FOREIGN KEY (Author_id) REFERENCES Author(Id)\\n);\\n\\n-- Table: JidoukaTool\\nCREATE TABLE JidoukaTool (\\n    Jidouka_id BIGINT,\\n    Tool_id INT,\\n    PRIMARY KEY (Jidouka_id, Tool_id),\\n    FOREIGN KEY (Jidouka_id) REFERENCES Jidouka(Id),\\n    FOREIGN KEY (Tool_id) REFERENCES Tool(Id)\\n);<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat are the names of all jobs created in the database?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSELECT Job_name FROM Job;<|eot_id|>\""},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=10,\n    eval_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:15:21.405064Z","iopub.execute_input":"2024-11-19T09:15:21.405587Z","iopub.status.idle":"2024-11-19T09:15:21.441775Z","shell.execute_reply.started":"2024-11-19T09:15:21.405548Z","shell.execute_reply":"2024-11-19T09:15:21.440870Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset_train,\n    eval_dataset=dataset_valid,\n    peft_config=peft_config,\n    max_seq_length=512,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    dataset_text_field='messages',\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:15:23.853125Z","iopub.execute_input":"2024-11-19T09:15:23.853506Z","iopub.status.idle":"2024-11-19T09:15:25.012214Z","shell.execute_reply.started":"2024-11-19T09:15:23.853473Z","shell.execute_reply":"2024-11-19T09:15:25.011506Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/456 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c75ae0e7de74c72b0441e6abe6bfd44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/51 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a9f7d90668542fba2affdd1831a1059"}},"metadata":{}}]},{"cell_type":"code","source":"eot = \"<|eot_id|>\"\neot_id = tokenizer.convert_tokens_to_ids(eot)\ntokenizer.pad_token = eot\ntokenizer.pad_token_id = eot_id","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:15:27.893977Z","iopub.execute_input":"2024-11-19T09:15:27.894718Z","iopub.status.idle":"2024-11-19T09:15:27.899530Z","shell.execute_reply.started":"2024-11-19T09:15:27.894681Z","shell.execute_reply":"2024-11-19T09:15:27.898644Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:15:34.853641Z","iopub.execute_input":"2024-11-19T09:15:34.854035Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1902' max='2280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1902/2280 55:12 < 10:59, 0.57 it/s, Epoch 8.34/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>456</td>\n      <td>0.002700</td>\n      <td>0.002733</td>\n    </tr>\n    <tr>\n      <td>912</td>\n      <td>0.002600</td>\n      <td>0.002721</td>\n    </tr>\n    <tr>\n      <td>1368</td>\n      <td>0.003300</td>\n      <td>0.002718</td>\n    </tr>\n    <tr>\n      <td>1824</td>\n      <td>0.002700</td>\n      <td>0.002717</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = 'llama-3.2-1b-sql_finetuned_multitableJidouka_1.0_adapter'\nnew_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = 'huyhoangt2201/llama-3.2-1b-sql_finetuned_multitableJidouka_1.0_adapter\nbase_model = 'phamhai/Llama-3.2-1B-Instruct-Frog'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model)\n\nbase_model_reload = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        return_dict=True,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\n\n# base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n\n# Merge adapter with base model\nmerge_model = PeftModel.from_pretrained(base_model_reload, new_model)\n\nmerge_model = merge_model.merge_and_unload()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model_merged = 'llama-3.2-1b-sql_finetuned_mutitableJidouka_1.0_merged'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merge_model.save_pretrained(new_model_merged)\ntokenizer.save_pretrained(new_model_merged)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token = hf_token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merge_model.push_to_hub(new_model_merged, use_temp_dir=False)\ntokenizer.push_to_hub(new_model_merged, use_temp_dir=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## new_model inference","metadata":{}},{"cell_type":"code","source":"new_model_name = 'huyhoangt2201/llama-3.2-1b-sql_finetuned_billingual_2.0_merged'","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:46:41.120277Z","iopub.execute_input":"2024-11-14T08:46:41.120685Z","iopub.status.idle":"2024-11-14T08:46:41.125584Z","shell.execute_reply.started":"2024-11-14T08:46:41.120650Z","shell.execute_reply":"2024-11-14T08:46:41.124566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\ndevice = torch.device('cuda')\nmodel_path = new_model_name\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path).to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:46:45.698419Z","iopub.execute_input":"2024-11-14T08:46:45.698799Z","iopub.status.idle":"2024-11-14T08:47:50.108396Z","shell.execute_reply.started":"2024-11-14T08:46:45.698763Z","shell.execute_reply":"2024-11-14T08:47:50.107407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_template = \"\"\"\nYou are an SQL query assistant. Based on the table information below, generate an SQL query to retrieve the relevant information for the user. If the user’s question is unrelated to the table, respond naturally in human language.\n\nThe jidouka table contains the following columns:\nid: Row identifier (int)\ntên_cải_tiến: Name of the improvement (str)\nloại_hình_công_việc: Type of work that the improvement is intended to enhance (str) (e.g., database processing, data entry, workflow optimization, etc.)\ncông_cụ: Tool used to achieve the improvement (str) (e.g., Python, Excel, Visual Studio Code, etc.)\nmô_tả: Detailed description of the improvement (str) (e.g., each step of the improvement process)\nsản_phẩm: Output product of the improvement (str) (e.g., .csv file, .xlsx file, etc.)\ntác_giả: Contributor, company employee, or creator of the improvement (str)\nbộ_phận: Department of the author, usually referred to as \"dc\" (str) (e.g., dc1, dc2, dc3, dcd, souko, etc.)\nsố_giờ: Number of hours saved by applying the improvement (int)\nsố_công_việc_áp_dụng: Number of tasks in the company that the improvement has supported (int)\nthời_điểm_ra_mắt: Launch date of the tool (str) (e.g., 2024-10-11, 2024-10-09, etc.)\nthông_tin_thêm: Link to additional documentation (PowerPoint, video) on using the improvement or the improvement’s tool (str)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:47:53.911749Z","iopub.execute_input":"2024-11-14T08:47:53.912137Z","iopub.status.idle":"2024-11-14T08:47:53.918147Z","shell.execute_reply.started":"2024-11-14T08:47:53.912100Z","shell.execute_reply":"2024-11-14T08:47:53.916971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List, Dict\nclass ContextAwareChatbot:\n    def __init__(self,prompt, max_history: int = 5):\n        self.model = model\n        self.tokenizer = tokenizer \n        self.max_history = max_history\n        self.conversation_history: List[Dict[str, str]] = []\n        self.prompt=prompt\n    def _build_prompt(self) -> str:\n        # Build context from history\n\n        return self.prompt\n\n    def _clean_response(self, response: str) -> str:\n        # Clean up the generated response\n        response = response.split(\"Assistant:\")[-1].strip()\n        # Stop at any new \"Human:\" or \"Assistant:\" markers\n        if \"Human:\" in response:\n            response = response.split(\"Human:\")[0].strip()\n        return response\n\n    def chat(self, user_input: str) -> str:\n        # Generate the contextualized prompt\n        prompt = self._build_prompt()\n\n#         # Generate response\n#         response = self.pipeline(\n#             prompt,\n#             return_full_text=False,\n#             clean_up_tokenization_spaces=True\n#         )[0]['generated_text']\n\n#         # Clean the response\n#         cleaned_response = self._clean_response(response)\n        messages =[\n            {'role':'system',\n             'content':prompt}\n            ,\n            {'role':'user',\n             'content':user_input}\n        ]\n        tokenized_chat = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors='pt').to('cuda')\n        outputs = self.model.generate(tokenized_chat, max_new_tokens=256).to('cuda')\n        bot_response = self.tokenizer.decode(outputs[0])\n        bot_response = bot_response.split('<|start_header_id|>assistant<|end_header_id|>')\n        bot_response = bot_response[1].strip()[:-10]\n        # Update conversation history\n        self.conversation_history.append({\n            'human': user_input,\n            'assistant': bot_response\n        })\n\n        return bot_response\n\n    def get_history(self) -> List[Dict[str, str]]:\n        return self.conversation_history\n\n    def clear_history(self):\n        self.conversation_history = []\n\n# 4. Create chatbot instance\nchatbot = ContextAwareChatbot(prompt_template)\n\n# 5. Example usage function\ndef chat_session():\n    print(\"Chatbot initialized. Type 'exit' to end the conversation, 'clear' to clear history.\")\n\n    while True:\n        user_input = input(\"\\nYou: \").strip()\n\n        if user_input.lower() == 'exit':\n            print(\"Goodbye!\")\n            break\n        elif user_input.lower() == 'clear':\n            chatbot.clear_history()\n            print(\"Conversation history cleared!\")\n            continue\n\n        response = chatbot.chat(user_input)\n        print(f\"\\nAssistant: {response}\")\n\n# 6. Example of how to use\nif __name__ == \"__main__\":\n    chat_session()","metadata":{"execution":{"iopub.status.busy":"2024-11-14T08:47:59.969801Z","iopub.execute_input":"2024-11-14T08:47:59.970176Z","iopub.status.idle":"2024-11-14T08:52:29.918031Z","shell.execute_reply.started":"2024-11-14T08:47:59.970141Z","shell.execute_reply":"2024-11-14T08:52:29.916650Z"},"trusted":true},"execution_count":null,"outputs":[]}]}